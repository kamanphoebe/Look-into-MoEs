{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Static Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes experiments listed below:\n",
    "- Weight matrices of experts\n",
    "    - Matrix-level\n",
    "    - Neuron-level (averaging and reordering)\n",
    "- Gate embedding \n",
    "    - Qualitative\n",
    "    - Quantitative (linear regression)\n",
    "- Projection of expert matrices in low-dimensional space\n",
    "    - Matrix-level\n",
    "    - Neuron-level\n",
    "\n",
    "The models have their own code blocks for each experiment. The overall logic of the code belonging to different models is alike, and the minor differences stem from the unique settings of the corresponding model.\n",
    "\n",
    "Usually, the figures are plotted in two ways: 'auto_colorbar' and 'full_colorbar'. The former allows the matplotlib methods to automatically dicide the range of the color bar for each layer. For the latter, we manually set it to be the global minimum/maximum for all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import ml_dtypes\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mixtral_base.modeling_moe_mistral import MixtralForCausalLM\n",
    "from mistral.modeling_mistral import MistralModel\n",
    "from deepseekmoe.modeling_deepseek import DeepseekForCausalLM\n",
    "from grok.modeling_grok1 import Grok1ModelForCausalLM\n",
    "\n",
    "# The root directory for saving the output figures and data.\n",
    "WORK_DIR = './outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one or more cells below to load the models you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_model = MixtralForCausalLM.from_pretrained(\n",
    "    \"./ckpt/mixtral\", \n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "mixtral_tok = AutoTokenizer.from_pretrained(\"./ckpt/mixtral\")\n",
    "mixtral_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = MistralModel.from_pretrained(\n",
    "    './ckpt/mistral',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "mistral_tok = AutoTokenizer.from_pretrained(\"./ckpt/mistral\")\n",
    "mistral_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_model = DeepseekForCausalLM.from_pretrained(\n",
    "    './ckpt/deepseekmoe',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "deepseek_tok = AutoTokenizer.from_pretrained(\"./ckpt/deepseekmoe\")\n",
    "deepseek_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_model = Grok1ModelForCausalLM.from_pretrained(\n",
    "    './ckpt/grok',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "grok_tok = AutoTokenizer.from_pretrained(\"./ckpt/grok\")\n",
    "grok_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Matrices of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the code of both **matrix-level** and **neuron-level** analyses. We propose two methods for the neuron-level analyses: averaging and reordering. The code of matrix-level part and averaging is identical, except that you have to set `average=True` for the latter. Note that the reordering approach takes considerable time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level / Neuron-level (averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = [('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "num_neurons = mixtral_model.config.intermediate_size\n",
    "average = False # True for neuron-level.\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(num_layers):\n",
    "    for mix_mat, mis_mat in matrices:\n",
    "        if average:\n",
    "            mean_dim = 1 if mix_mat == 'w2' else 0\n",
    "            all_matrix = [torch.mean(getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight, dim=mean_dim)\n",
    "                           for idx in range(num_experts)]\n",
    "            all_matrix.append(torch.mean(getattr(mistral_model.layers[i].mlp, mis_mat).weight, dim=mean_dim))\n",
    "        else:\n",
    "            all_matrix = [getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.flatten() \n",
    "                           for idx in range(num_experts)]\n",
    "            all_matrix.append(getattr(mistral_model.layers[i].mlp, mis_mat).weight.flatten())\n",
    "        sim_arr = np.empty((num_experts+1, num_experts+1))\n",
    "        for j in range(num_experts+1):\n",
    "            for k in range(j, num_experts+1):\n",
    "                # Mixtral and Mistral layers can be loaded on differnet GPUs, so put them on the same device manually. \n",
    "                sim = cos(all_matrix[j].to('cuda:0'), all_matrix[k].to('cuda:0')).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        # Record the maximum and minimum values for plotting.\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "tick_labels.append('F')\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_experts_sim')\n",
    "if average:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dict = {'global_vmax': global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(6., 1.85))\n",
    "    imlst = []\n",
    "    for l, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[l].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[l].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        axs[l].set_xticks(np.arange(num_experts+1), labels=tick_labels, fontsize=13)\n",
    "        axs[l].set_yticks(np.arange(num_experts+1), labels=tick_labels, fontsize=13)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=10., fontsize=16)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.85)\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level (reordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "cos_dim0 = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_dim1 = torch.nn.CosineSimilarity(dim=1)\n",
    "matrices = [('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "num_neurons = mixtral_model.config.intermediate_size\n",
    "save_dir = os.path.join(WORK_DIR, f'mixtral/mixtral_experts_sim_reorder')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_permute_idx(mat1, mat2, mat_type):\n",
    "    # Find the permutation of mat2 which maximizes the \n",
    "    # cosine similarity between mat1 and mat2.\n",
    "    all_neuron_sim = np.empty((num_neurons, num_neurons))\n",
    "    for i in range(num_neurons):\n",
    "        neuron1 = mat1[:, i] if mat_type == 'down_proj' else mat1[i, :]\n",
    "        if mat_type == 'down_proj':\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "        else:\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2.T).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "    _, col_idx = linear_sum_assignment(all_neuron_sim, maximize=True)\n",
    "    return col_idx\n",
    "\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "all_kerror_arr = [[] for _ in range(num_layers)]\n",
    "layers = [0, 5, 10, 15, 20, 25, 30, 31]\n",
    "for i in layers:\n",
    "    print(f'Layer {i}')\n",
    "    for m, (mix_mat, mis_mat) in enumerate(matrices):\n",
    "        print(f'{mix_mat}/{mis_mat}')\n",
    "        all_matrix = [getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight\n",
    "                           for idx in range(num_experts)]\n",
    "        mistral_matrix = getattr(mistral_model.layers[i].mlp, mis_mat).weight\n",
    "        all_matrix.append(mistral_matrix)\n",
    "        sim_arr = np.empty((num_experts+1, num_experts+1))\n",
    "        kerror_arr = np.empty((num_experts+1, num_experts+1))\n",
    "        for j in range(num_experts+1):\n",
    "            kerror_lst = [[] for _ in range(len(matrices))]\n",
    "            for k in range(j, num_experts+1):\n",
    "                if j == k:\n",
    "                    max_sim = 1.\n",
    "                    kerror = 1.\n",
    "                else:\n",
    "                    permute_idx = get_permute_idx(all_matrix[j], all_matrix[k], mis_mat)\n",
    "                    kerror = kendalltau(permute_idx, np.arange(num_neurons)).statistic\n",
    "                    permute_mat = all_matrix[k][:, permute_idx] if mis_mat == 'down_proj' else all_matrix[k][permute_idx]\n",
    "                    cos = cos_dim0 if mis_mat == 'down_proj' else cos_dim1\n",
    "                    max_sim = cos(all_matrix[j].flatten(), permute_mat.flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # ori_sim = cos_dim0(all_matrix[j].flatten(), all_matrix[k].flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # print(max_sim - ori_sim)\n",
    "                sim_arr[j][k] = max_sim\n",
    "                sim_arr[k][j] = max_sim\n",
    "                kerror_arr[j][k] = kerror\n",
    "                kerror_arr[k][j] = kerror\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        all_kerror_arr[i].append(kerror_arr)\n",
    "\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_sim_arr'), 'wb') as f:\n",
    "            pickle.dump(all_sim_arr[i], f)\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_kerror_arr'), 'wb') as f:\n",
    "            pickle.dump(all_kerror_arr[i], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level / Neuron-level (averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "average = False # True for neuron-level.\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(1, num_layers): # The first layer is a FFN layer.\n",
    "    for matrix in matrices:\n",
    "        num_total_experts = num_routed_experts\n",
    "        if average:\n",
    "            mean_dim = 1 if matrix == 'down_proj' else 0\n",
    "            all_experts = [torch.mean(getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight, dim=mean_dim)\n",
    "                           for idx in range(num_routed_experts)]\n",
    "            # Include the shared experts if measuring neuron-level similarity.\n",
    "            all_experts.append(torch.mean(getattr(deepseek_model.model.layers[i].mlp.shared_experts, matrix).weight, dim=mean_dim))\n",
    "            num_total_experts = num_total_experts + 1\n",
    "        else:\n",
    "            all_experts = [getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight.flatten() \n",
    "                           for idx in range(num_routed_experts)]\n",
    "        sim_arr = np.empty((num_total_experts, num_total_experts))\n",
    "        for j in range(num_total_experts):\n",
    "            for k in range(j, num_total_experts):\n",
    "                sim = cos(all_experts[j], all_experts[k]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        # Record the maximum and minimum values for plotting.\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "tick_pos = [i for i in range(0, num_routed_experts, 8)]\n",
    "tick_labels = [str(i) for i in range(0, num_routed_experts, 8)]\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_experts_sim')\n",
    "if average:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "output_dict = {'global_vmax': global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(12., 3.7))\n",
    "    imlst = []\n",
    "    for l, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[l].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[l].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        curr_tick_pos = tick_pos.copy()\n",
    "        curr_tick_labels = tick_labels.copy()\n",
    "        if average:\n",
    "            curr_tick_pos.append(num_routed_experts)\n",
    "            curr_tick_labels.append('SE')\n",
    "        axs[l].set_xticks(curr_tick_pos, labels=curr_tick_labels, fontsize=16)\n",
    "        axs[l].set_yticks(curr_tick_pos, labels=curr_tick_labels, fontsize=16)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=20., fontsize=32)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.83)\n",
    "    cbar.ax.tick_params(labelsize=22)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level (reordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "cos_dim0 = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_dim1 = torch.nn.CosineSimilarity(dim=1)\n",
    "matrices = ['up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "num_neurons = deepseek_model.config.moe_intermediate_size\n",
    "save_dir = os.path.join(WORK_DIR, f'deepseek/deepseek_experts_sim_reorder')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_permute_idx(mat1, mat2, mat_type):\n",
    "    # Find the permutation of mat2 which maximizes the \n",
    "    # cosine similarity between mat1 and mat2.\n",
    "    all_neuron_sim = np.empty((num_neurons, num_neurons))\n",
    "    for i in range(num_neurons):\n",
    "        neuron1 = mat1[:, i] if mat_type == 'down_proj' else mat1[i, :]\n",
    "        if mat_type == 'down_proj':\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "        else:\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2.T).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "    _, col_idx = linear_sum_assignment(all_neuron_sim, maximize=True)\n",
    "    return col_idx\n",
    "                      \n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "all_kerror_arr = [[] for _ in range(num_layers)]\n",
    "layers = [1, 5, 10, 15, 20, 25, 27]\n",
    "for i in layers:\n",
    "    print(f'Layer {i}')\n",
    "    all_sim_arr = []\n",
    "    for matrix in matrices:\n",
    "        print(matrix)\n",
    "        all_matrix = [getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight\n",
    "                           for idx in range(num_routed_experts)]\n",
    "        all_matrix.append(getattr(deepseek_model.model.layers[i].mlp.shared_experts, matrix).weight)\n",
    "        sim_arr = np.empty((num_routed_experts+1, num_routed_experts+1))\n",
    "        kerror_arr = np.empty((num_routed_experts+1, num_routed_experts+1))\n",
    "        for j in range(num_routed_experts+1):\n",
    "            kerror_lst = [[] for _ in range(len(matrices))]\n",
    "            for k in range(j, num_routed_experts+1):\n",
    "                if j == k:\n",
    "                    max_sim = 1.\n",
    "                    kerror = 1.\n",
    "                else:\n",
    "                    permute_idx = get_permute_idx(all_matrix[j], all_matrix[k], matrix)\n",
    "                    kerror = kendalltau(permute_idx, np.arange(num_neurons)).statistic\n",
    "                    permute_mat = all_matrix[k][:, permute_idx] if matrix == 'down_proj' else all_matrix[k][permute_idx]\n",
    "                    cos = cos_dim0 if matrix == 'down_proj' else cos_dim1\n",
    "                    max_sim = cos(all_matrix[j].flatten(), permute_mat.flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # ori_sim = cos_dim0(all_matrix[j].flatten(), all_matrix[k].flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # print(max_sim - ori_sim)\n",
    "                sim_arr[j][k] = max_sim\n",
    "                sim_arr[k][j] = max_sim\n",
    "                kerror_arr[j][k] = kerror\n",
    "                kerror_arr[k][j] = kerror\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        all_kerror_arr[i].append(kerror_arr)\n",
    "\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_sim_arr'), 'wb') as f:\n",
    "            pickle.dump(all_sim_arr[i], f)\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_kerror_arr'), 'wb') as f:\n",
    "            pickle.dump(all_kerror_arr[i], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level / Neuron-level (averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['linear_v', 'linear', 'linear_1'] # up, gate, down\n",
    "average = False # True for neuron-level.\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(num_layers):\n",
    "    for matrix in matrices:\n",
    "        if average:\n",
    "            mean_dim = 1 if matrix == 'linear_1' else 0\n",
    "            all_experts = [torch.mean(getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight, dim=mean_dim) \n",
    "                           for idx in range(num_experts)]\n",
    "        else:\n",
    "            all_experts = [getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight.flatten() \n",
    "                           for idx in range(num_experts)]\n",
    "        sim_arr = np.empty((num_experts, num_experts))\n",
    "        for j in range(num_experts):\n",
    "            # Calculate similarity between Grok experts.\n",
    "            for k in range(j, num_experts):\n",
    "                sim = cos(all_experts[j], all_experts[k]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        # Record the maximum and minimum values for plotting.\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_experts_sim')\n",
    "if average:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "\n",
    "output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(6., 1.85))\n",
    "    imlst = []\n",
    "    for l, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[l].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[l].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        axs[l].set_xticks(np.arange(num_experts), labels=tick_labels, fontsize=14)\n",
    "        axs[l].set_yticks(np.arange(num_experts), labels=tick_labels, fontsize=14)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=10., fontsize=16)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.85)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level (reordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "cos_dim0 = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_dim1 = torch.nn.CosineSimilarity(dim=1)\n",
    "matrices = ['linear_v', 'linear', 'linear_1'] # up, gate, down\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "num_neurons = grok_model.config.intermediate_size\n",
    "save_dir = os.path.join(WORK_DIR, f'grok/grok_experts_sim_reorder')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_permute_idx(mat1, mat2, mat_type):\n",
    "    # Find the permutation of mat2 which maximizes the \n",
    "    # cosine similarity between mat1 and mat2.\n",
    "    all_neuron_sim = np.empty((num_neurons, num_neurons))\n",
    "    for i in range(num_neurons):\n",
    "        neuron1 = mat1[:, i] if mat_type == 'linear_1' else mat1[i, :]\n",
    "        if mat_type == 'linear_1':\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)  \n",
    "        else:\n",
    "            all_neuron_sim[i] = cos_dim0(neuron1.unsqueeze(1), mat2.T).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "    _, col_idx = linear_sum_assignment(all_neuron_sim, maximize=True)\n",
    "    with open(os.path.join(output_dir, 'record.txt'), 'a+') as f:\n",
    "        print(f'permute_idx: {col_idx}', file=f)\n",
    "    return col_idx\n",
    "\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "all_kerror_arr = [[] for _ in range(num_layers)]\n",
    "layers = [0]\n",
    "for i in layers:\n",
    "    print(f'Layer {i}')\n",
    "    for matrix in matrices:\n",
    "        print(matrix)\n",
    "        all_matrix = [getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight \n",
    "                      for idx in range(num_experts)]\n",
    "        sim_arr = np.empty((num_experts, num_experts))\n",
    "        kerror_arr = np.empty((num_experts, num_experts))\n",
    "        for j in range(num_experts):\n",
    "            kerror_lst = [[] for _ in range(len(matrices))]\n",
    "            for k in range(j, num_experts):\n",
    "                if j == k:\n",
    "                    max_sim = 1.\n",
    "                    kerror = 1.\n",
    "                else:\n",
    "                    permute_idx = get_permute_idx(all_matrix[j], all_matrix[k], matrix)\n",
    "                    kerror = kendalltau(permute_idx, np.arange(num_neurons)).statistic\n",
    "                    permute_mat = all_matrix[k][:, permute_idx] if matrix == 'linear_1' else all_matrix[k][permute_idx]\n",
    "                    cos = cos_dim0 if matrix == 'down_proj' else cos_dim1\n",
    "                    max_sim = cos(all_matrix[j].flatten(), permute_mat.flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # ori_sim = cos_dim0(all_matrix[j].flatten(), all_matrix[k].flatten()).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                # print(max_sim - ori_sim)\n",
    "                sim_arr[j][k] = max_sim\n",
    "                sim_arr[k][j] = max_sim\n",
    "                kerror_arr[j][k] = kerror\n",
    "                kerror_arr[k][j] = kerror\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        all_kerror_arr[i].append(kerror_arr)\n",
    "\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_sim_arr'), 'wb') as f:\n",
    "            pickle.dump(all_sim_arr[i], f)\n",
    "        with open(os.path.join(output_dir, f'layer{i}_all_kerror_arr'), 'wb') as f:\n",
    "            pickle.dump(all_kerror_arr[i], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gate Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have conducted both qualitative and quantitative analysis for the gate embedding. To run this experiment, you have to first execute the 1st code block to compute the similarities, then you can plot the heat map (qualitative) and/or perform the linear regression (quantitative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral and Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = [('gate', 'gate'), ('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "average = True\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(num_layers):\n",
    "    # Calcualte similarity between neurons in gate embedding.\n",
    "    gate = mixtral_model.model.layers[i].mlp.gate.weight\n",
    "    sim_arr = np.empty((num_experts, num_experts))\n",
    "    for j in range(num_experts):\n",
    "        for k in range(j, num_experts):\n",
    "            sim = cos(gate[j, :], gate[k, :]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)  \n",
    "            sim_arr[j][k] = sim\n",
    "            sim_arr[k][j] = sim\n",
    "    all_sim_arr[i].append(sim_arr)\n",
    "    curr_vmax = np.max(sim_arr)\n",
    "    curr_vmin = np.min(sim_arr)\n",
    "    if curr_vmin < global_vmin:\n",
    "        global_vmin = curr_vmin\n",
    "    if curr_vmax > global_vmax:\n",
    "        global_vmax = curr_vmax\n",
    "    # Calculate similarity between Mixtral experts.\n",
    "    for mix_mat, _ in matrices[1:]:\n",
    "        if average:\n",
    "            mean_dim = 1 if mix_mat == 'w2' else 0\n",
    "            all_experts = [torch.mean(getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight, dim=mean_dim)\n",
    "                           for idx in range(num_experts)]\n",
    "        else:\n",
    "            all_experts = [getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.flatten() \n",
    "                           for idx in range(num_experts)]\n",
    "        sim_arr = np.empty((num_experts, num_experts))\n",
    "        for j in range(num_experts):\n",
    "            for k in range(j, num_experts):\n",
    "                sim = cos(all_experts[j], all_experts[k]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)  \n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative analysis (plotted along with the neuron-level heat maps of expert weight matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_gate_sim')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "\n",
    "output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "    \n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    imlst = []\n",
    "    fig, axs = plt.subplots(ncols=4, layout='constrained', figsize=(8.5, 2.0))\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        axs[i].set_xticks(np.arange(num_experts), labels=tick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(np.arange(num_experts), labels=tick_labels, fontsize=15)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatitative analysis (linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_gate_sim_reg')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize the similarity matrices to be one-to-one value pairs.\n",
    "all_data = [[np.array([]) for _ in range(len(matrices))] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    for j, sim_arr in enumerate(all_sim_arr[i]):\n",
    "        # Iterate over the similarity array to flatten the \n",
    "        # low triangle area (excluding the diagonal).\n",
    "        for row in range(num_experts):\n",
    "            for col in range(row):\n",
    "                all_data[i][j] = np.append(all_data[i][j], sim_arr[row][col])\n",
    "\n",
    "# Perform linear regression.\n",
    "sum_r2 = [0. for _ in range(len(matrices))]\n",
    "all_r_lst = [[] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    for j in range(1, len(matrices)):\n",
    "        X, Y = all_data[i][j], all_data[i][0]\n",
    "        slope, intercept, r, p, stderr = linregress(X, Y)\n",
    "        r2 = round(r**2, 2)\n",
    "        sum_r2[j] += r2\n",
    "        all_r_lst[i].append(r)\n",
    "\n",
    "print('Average regression score\\nup_proj: {:.2f}\\ngate_proj: {:.2f}\\ndown_proj: {:.2f}'.format(\n",
    "    sum_r2[1]/num_layers, sum_r2[2]/num_layers, sum_r2[3]/num_layers))\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_data'), 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "with open(os.path.join(output_dir, 'all_r_lst'), 'wb') as f:\n",
    "    pickle.dump(all_r_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['gate', 'up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "average = True\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)]\n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(1, num_layers): # The first layer is a FFN layer.\n",
    "    # Calcualte similarity between neurons in gate embedding.\n",
    "    gate = deepseek_model.model.layers[i].mlp.gate.weight\n",
    "    sim_arr = np.empty((num_routed_experts, num_routed_experts))\n",
    "    for j in range(num_routed_experts):\n",
    "        for k in range(j, num_routed_experts):\n",
    "            sim = cos(gate[j, :], gate[k, :]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "            sim_arr[j][k] = sim\n",
    "            sim_arr[k][j] = sim\n",
    "    all_sim_arr[i].append(sim_arr)\n",
    "    curr_vmax = np.max(sim_arr)\n",
    "    curr_vmin = np.min(sim_arr)\n",
    "    if curr_vmin < global_vmin:\n",
    "        global_vmin = curr_vmin\n",
    "    if curr_vmax > global_vmax:\n",
    "        global_vmax = curr_vmax\n",
    "    # Calculate similarity between DeepSeek routed experts.\n",
    "    for matrix in matrices[1:]:\n",
    "        if average:\n",
    "            mean_dim = 1 if matrix == 'down_proj' else 0\n",
    "            all_experts = [torch.mean(getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight, dim=mean_dim)\n",
    "                           for idx in range(num_routed_experts)]\n",
    "        else:\n",
    "            all_experts = [getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight.flatten() \n",
    "                           for idx in range(num_routed_experts)]\n",
    "        sim_arr = np.empty((num_routed_experts, num_routed_experts))\n",
    "        for j in range(num_routed_experts):\n",
    "            for k in range(j, num_routed_experts):\n",
    "                sim = cos(all_experts[j], all_experts[k]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative analysis (plotted along with the neuron-level heat maps of expert weight matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "tick_pos = [i for i in range(0, num_routed_experts, 8)]\n",
    "tick_labels = [str(i) for i in range(0, num_routed_experts, 8)]\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_gate_sim')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    imlst = []\n",
    "    fig, axs = plt.subplots(ncols=4, layout='constrained', figsize=(15.4, 3.5))\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        axs[i].set_xticks(tick_pos, tick_labels, fontsize=18)\n",
    "        axs[i].set_yticks(tick_pos, tick_labels, fontsize=18)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=18., fontsize=30)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.88)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatitative analysis (linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_gate_sim_reg')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize the similarity matrices to be one-to-one value pairs.\n",
    "all_data = [[np.array([]) for _ in range(len(matrices))] for _ in range(num_layers)]\n",
    "for i in range(1, num_layers):\n",
    "    for j, sim_arr in enumerate(all_sim_arr[i]):\n",
    "        # Iterate over the similarity array to flatten the \n",
    "        # low triangle area (excluding the diagonal).\n",
    "        for row in range(num_routed_experts):\n",
    "            for col in range(row):\n",
    "                all_data[i][j] = np.append(all_data[i][j], sim_arr[row][col])\n",
    "\n",
    "# Perform linear regression.\n",
    "sum_r2 = [0. for _ in range(len(matrices))]\n",
    "all_r_lst = [[] for _ in range(num_layers)]\n",
    "for i in range(1, num_layers):\n",
    "    for j in range(1, len(matrices)):\n",
    "        X, Y = all_data[i][j], all_data[i][0]\n",
    "        slope, intercept, r, p, stderr = linregress(X, Y)\n",
    "        r2 = round(r**2, 2)\n",
    "        sum_r2[j] += r2\n",
    "        all_r_lst[i].append(r)\n",
    "\n",
    "print('Average regression score\\nup_proj: {:.2f}\\ngate_proj: {:.2f}\\ndown_proj: {:.2f}'.format(\n",
    "    sum_r2[1]/(num_layers-1), sum_r2[2]/(num_layers-1), sum_r2[3]/(num_layers-1)))\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_data'), 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "with open(os.path.join(output_dir, 'all_r_lst'), 'wb') as f:\n",
    "    pickle.dump(all_r_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['gate', 'linear_v', 'linear', 'linear_1']\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "average = True\n",
    "\n",
    "all_sim_arr = [[] for _ in range(num_layers)] \n",
    "global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "for i in range(num_layers):\n",
    "    # Calcualte similarity between neurons in gate embedding.\n",
    "    gate = grok_model.model.layers[i].moe_block.gate.weight\n",
    "    sim_arr = np.empty((num_experts, num_experts))\n",
    "    for j in range(num_experts):\n",
    "        for k in range(j, num_experts):\n",
    "            sim = cos(gate[j, :], gate[k, :]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "            sim_arr[j][k] = sim\n",
    "            sim_arr[k][j] = sim\n",
    "    all_sim_arr[i].append(sim_arr)\n",
    "    curr_vmax = np.max(sim_arr)\n",
    "    curr_vmin = np.min(sim_arr)\n",
    "    if curr_vmin < global_vmin:\n",
    "        global_vmin = curr_vmin\n",
    "    if curr_vmax > global_vmax:\n",
    "        global_vmax = curr_vmax\n",
    "    # Calculate similarity between Grok experts.\n",
    "    for matrix in matrices[1:]:\n",
    "        if average:\n",
    "            mean_dim = 1 if matrix == 'linear_1' else 0\n",
    "            all_experts = [torch.mean(getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight, dim=mean_dim) \n",
    "            for idx in range(num_experts)]\n",
    "        else:\n",
    "            all_experts = [getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight.flatten() \n",
    "                        for idx in range(num_experts)]\n",
    "        sim_arr = np.empty((num_experts, num_experts))\n",
    "        for j in range(num_experts):\n",
    "            for k in range(j, num_experts):\n",
    "                sim = cos(all_experts[j], all_experts[k]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                sim_arr[j][k] = sim\n",
    "                sim_arr[k][j] = sim\n",
    "        all_sim_arr[i].append(sim_arr)\n",
    "        curr_vmax = np.max(sim_arr)\n",
    "        curr_vmin = np.min(sim_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative analysis (plotted along with the neuron-level heat maps of expert weight matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_gate_sim')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "\n",
    "output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "    pickle.dump(all_sim_arr, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(arr_lst, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, axs = plt.subplots(ncols=4, layout='constrained', figsize=(8.5, 2.0))\n",
    "    imlst = []\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        axs[i].set_xticks(np.arange(num_experts), labels=tick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(np.arange(num_experts), labels=tick_labels, fontsize=15)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_sim_arr[i], i, 'auto_colorbar')\n",
    "    plot_one_layer(all_sim_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatitative analysis (linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_gate_sim_reg')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize the similarity matrices to be one-to-one value pairs.\n",
    "all_data = [[np.array([]) for _ in range(len(matrices))] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    for j, sim_arr in enumerate(all_sim_arr[i]):\n",
    "        # Iterate over the similarity array to flatten the \n",
    "        # low triangle area (excluding the diagonal).\n",
    "        for row in range(num_experts):\n",
    "            for col in range(row):\n",
    "                all_data[i][j] = np.append(all_data[i][j], sim_arr[row][col])\n",
    "\n",
    "# Perform linear regression.\n",
    "sum_r2 = [0. for _ in range(len(matrices))]\n",
    "all_r_lst = [[] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    for j in range(1, len(matrices)):\n",
    "        X, Y = all_data[i][j], all_data[i][0]\n",
    "        slope, intercept, r, p, stderr = linregress(X, Y)\n",
    "        r2 = round(r**2, 2)\n",
    "        sum_r2[j] += r2\n",
    "        all_r_lst[i].append(r)\n",
    "    \n",
    "print('Average regression score\\nup_proj: {:.2f}\\ngate_proj: {:.2f}\\ndown_proj: {:.2f}'.format(\n",
    "    sum_r2[1]/num_layers, sum_r2[2]/num_layers, sum_r2[3]/num_layers))\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_data'), 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "with open(os.path.join(output_dir, 'all_r_lst'), 'wb') as f:\n",
    "    pickle.dump(all_r_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection of Expert Matrices in Low-dimensional Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the PCA projection code in both the matrix level and neuron level. For the neuron level, you can set `n_dim=2` or `n_dim=3` to change the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrices = [('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "num_neurons = mixtral_model.config.intermediate_size\n",
    "hidden_size = mixtral_model.config.hidden_size\n",
    "use_normalize = True\n",
    "        \n",
    "all_projected_matrix = [[] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    print(i)\n",
    "    for mix_mat, mis_mat in matrices:\n",
    "        all_matrix = torch.empty(num_experts+1, num_neurons*hidden_size)\n",
    "        for idx in range(num_experts):\n",
    "            if mis_mat == 'down_proj':\n",
    "                all_matrix[idx] = getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.T.flatten()\n",
    "            else:\n",
    "                all_matrix[idx] = getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.flatten()\n",
    "        if mis_mat == 'down_proj':\n",
    "            all_matrix[-1] = getattr(mistral_model.layers[i].mlp, mis_mat).weight.T.flatten()\n",
    "        else:\n",
    "            all_matrix[-1] = getattr(mistral_model.layers[i].mlp, mis_mat).weight.flatten()\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_matrix, dim=0), torch.std(all_matrix, dim=0)\n",
    "            all_matrix = (all_matrix - mean) / std\n",
    "        pca = PCA(n_components=2, svd_solver='full')\n",
    "        projected_matrix = pca.fit_transform(all_matrix.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "        all_projected_matrix[i].append(projected_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_experts_pca')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_projected_matrix'), 'wb') as f:\n",
    "    pickle.dump(all_projected_matrix, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(projected_matrix_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.5))\n",
    "    for l, projected_matrix in enumerate(projected_matrix_lst):\n",
    "        mix_X = projected_matrix[:-1, 0]\n",
    "        mix_Y = projected_matrix[:-1, 1]\n",
    "        mis_X = projected_matrix[-1, 0]\n",
    "        mis_Y = projected_matrix[-1, 1]\n",
    "        axs[l].scatter(mix_X, mix_Y, marker='o', label='expert')\n",
    "        axs[l].scatter(mis_X, mis_Y, marker='^', label='FFN')\n",
    "        axs[l].legend(loc='best', fontsize=11)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=10., fontsize=16)\n",
    "        for i in range(num_experts):\n",
    "            axs[l].annotate(i, (mix_X[i], mix_Y[i]), fontsize=12)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_projected_matrix[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrices = [('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "num_neurons = mixtral_model.config.intermediate_size\n",
    "use_normalize = True\n",
    "n_dim = 2 # 2D or 3D space\n",
    "assert n_dim in [2, 3]\n",
    "\n",
    "save_dir = os.path.join(WORK_DIR, f'mixtral/mixtral_experts_pca_neuron/{n_dim}d')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_2d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        mix_X = projected_neuron[:-1*num_neurons, 0]\n",
    "        mix_Y = projected_neuron[:-1*num_neurons, 1]\n",
    "        mis_X = projected_neuron[-1*num_neurons:, 0]\n",
    "        mis_Y = projected_neuron[-1*num_neurons:, 1]\n",
    "        color = []\n",
    "        for i in range(num_experts+1):\n",
    "            color.extend([i]*num_neurons)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=10., fontsize=16)\n",
    "        axs[l].scatter(mix_X, mix_Y, marker='o', c=color[:-1*num_neurons], cmap=plt.cm.Spectral)\n",
    "        axs[l].scatter(mis_X, mis_Y, marker='^', c=color[-1*num_neurons:], cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_3d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7), subplot_kw=dict(projection='3d'))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        mix_X = projected_neuron[:-1*num_neurons, 0]\n",
    "        mix_Y = projected_neuron[:-1*num_neurons, 1]\n",
    "        mix_Z = projected_neuron[:-1*num_neurons, 2]\n",
    "        mis_X = projected_neuron[-1*num_neurons:, 0]\n",
    "        mis_Y = projected_neuron[-1*num_neurons:, 1]\n",
    "        mis_Z = projected_neuron[-1*num_neurons:, 2]\n",
    "        color = []\n",
    "        for i in range(num_experts+1):\n",
    "            color.extend([i]*num_neurons)\n",
    "        axs[l].scatter(mix_X, mix_Y, mix_Z, marker='o', c=color[:-1*num_neurons], cmap=plt.cm.Spectral)\n",
    "        axs[l].scatter(mis_X, mis_Y, mis_Z, marker='^', c=color[-1*num_neurons:], cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    print(i)\n",
    "    projected_neuron_lst = []\n",
    "    for mix_mat, mis_mat in matrices:\n",
    "        all_neuron_lst = []\n",
    "        for idx in range(num_experts):\n",
    "            if mis_mat == 'down_proj':\n",
    "                all_neuron_lst.append(getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.T.to('cuda:0'))\n",
    "            else:\n",
    "                all_neuron_lst.append(getattr(mixtral_model.model.layers[i].mlp.experts[idx], mix_mat).weight.to('cuda:0'))\n",
    "        if mis_mat == 'down_proj':\n",
    "            all_neuron_lst.append(getattr(mistral_model.layers[i].mlp, mis_mat).weight.T.to('cuda:0'))\n",
    "        else:\n",
    "            all_neuron_lst.append(getattr(mistral_model.layers[i].mlp, mis_mat).weight.to('cuda:0'))\n",
    "        all_neuron = torch.cat(all_neuron_lst, dim=0)\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_neuron, dim=0), torch.std(all_neuron, dim=0)\n",
    "            all_neuron = (all_neuron - mean) / std\n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        projected_neuron = pca.fit_transform(all_neuron.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "        projected_neuron_lst.append(projected_neuron)\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'layer{i}_projected_neuron_lst'), 'wb') as f:\n",
    "        pickle.dump(projected_neuron_lst, f)\n",
    "\n",
    "    if n_dim == 2:\n",
    "        plot_one_2d_layer(projected_neuron_lst, i)\n",
    "    elif n_dim == 3:\n",
    "        plot_one_3d_layer(projected_neuron_lst, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "matrices = ['up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "num_neurons = deepseek_model.config.moe_intermediate_size\n",
    "hidden_size = deepseek_model.config.hidden_size\n",
    "remove_noise = True # If True, remove some outliers before plotting.\n",
    "use_normalize = True\n",
    "\n",
    "all_projected_matrix = [[] for _ in range(num_layers)]\n",
    "all_cluster_labels = [[] for _ in range(num_layers)]\n",
    "for i in range(1, num_layers):\n",
    "    for matrix in matrices:\n",
    "        all_matrix = torch.empty(num_routed_experts, num_neurons*hidden_size)\n",
    "        for idx in range(num_routed_experts):\n",
    "            if matrix == 'down_proj':\n",
    "                all_matrix[idx] = getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight.T.flatten()\n",
    "            else:\n",
    "                all_matrix[idx] = getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight.flatten()\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_matrix, dim=0), torch.std(all_matrix, dim=0)\n",
    "            all_matrix = (all_matrix - mean) / std\n",
    "        pca = PCA(n_components=2, svd_solver='full')\n",
    "        projected_matrix = pca.fit_transform(all_matrix.cpu().detach().numpy())\n",
    "        if remove_noise:\n",
    "            cluster = DBSCAN(eps=50.).fit(projected_matrix)\n",
    "            cluster_labels = cluster.labels_\n",
    "            projected_matrix = projected_matrix[cluster_labels>-1, :] # -1 indicates noisy point.\n",
    "            all_cluster_labels[i].append(cluster_labels)\n",
    "        all_projected_matrix[i].append(projected_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if remove_noise:\n",
    "    save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_experts_pca/reduce')\n",
    "else:\n",
    "    save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_experts_pca/raw')\n",
    "plot_dir = os.path.join(save_dir, 'figure/pdf')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_projected_matrix'), 'wb') as f:\n",
    "    pickle.dump(all_projected_matrix, f)\n",
    "with open(os.path.join(output_dir, 'all_cluster_labels'), 'wb') as f:\n",
    "    pickle.dump(all_cluster_labels, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(projected_matrix_lst, layer_idx, cluster_labels_lst=[]):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.5))\n",
    "    for l, projected_matrix in enumerate(projected_matrix_lst):\n",
    "        X = projected_matrix[:, 0]\n",
    "        Y = projected_matrix[:, 1]\n",
    "        axs[l].scatter(X, Y, marker='o')\n",
    "        subtitle = '('\n",
    "        count = 0\n",
    "        j = 0\n",
    "        for i in range(num_routed_experts):\n",
    "            if cluster_labels_lst and cluster_labels_lst[l][i] == -1:\n",
    "                subtitle += f'{i}, '\n",
    "                count += 1\n",
    "                if count == 8:\n",
    "                    subtitle += '\\n'\n",
    "                    count = 0\n",
    "                continue\n",
    "            axs[l].annotate(i, (X[j], Y[j]))\n",
    "            j += 1\n",
    "        subtitle += ')'\n",
    "        if remove_noise:\n",
    "            axs[l].set_title(f'{subtitle}', fontsize=10)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=10., fontsize=16)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    plot_one_layer(all_projected_matrix[i], i, all_cluster_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrices = ['up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "num_neurons = deepseek_model.config.moe_intermediate_size\n",
    "hidden_size = deepseek_model.config.hidden_size\n",
    "use_normalize = True\n",
    "n_dim = 2 # 2D or 3D space\n",
    "assert n_dim in [2, 3]\n",
    "\n",
    "all_projected_neuron = [[] for _ in range(num_layers)]\n",
    "all_cluster_labels = [[] for _ in range(num_layers)]\n",
    "for i in range(1, num_layers):\n",
    "    for matrix in matrices:\n",
    "        all_neuron_lst = []\n",
    "        for idx in range(num_routed_experts):\n",
    "            if matrix == 'down_proj':\n",
    "                all_neuron_lst.append(getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight.T)\n",
    "            else:\n",
    "                all_neuron_lst.append(getattr(deepseek_model.model.layers[i].mlp.experts[idx], matrix).weight)\n",
    "        # all_neuron.shape = (num_routed_experts*num_neurons, hidden_size)\n",
    "        all_neuron = torch.cat(all_neuron_lst, dim=0)\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_neuron, dim=0), torch.std(all_neuron, dim=0)\n",
    "            all_neuron = (all_neuron - mean) / std\n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        projected_neuron = pca.fit_transform(all_neuron.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "        all_projected_neuron[i].append(projected_neuron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, f'deepseek/deepseek_experts_pca_neuron/{n_dim}d')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_projected_neuron'), 'wb') as f:\n",
    "    pickle.dump(all_projected_neuron, f)\n",
    "with open(os.path.join(output_dir, 'all_cluster_labels'), 'wb') as f:\n",
    "    pickle.dump(all_cluster_labels, f)\n",
    "\n",
    "\n",
    "def plot_one_2d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        X = projected_neuron[:, 0]\n",
    "        Y = projected_neuron[:, 1]\n",
    "        color = []\n",
    "        for i in range(num_routed_experts):\n",
    "            color.extend([i]*num_neurons)\n",
    "        if l == 0:\n",
    "            axs[l].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "        axs[l].scatter(X, Y, marker='o', c=color, cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_3d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7), subplot_kw=dict(projection='3d'))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        X = projected_neuron[:, 0]\n",
    "        Y = projected_neuron[:, 1]\n",
    "        Z = projected_neuron[:, 2]\n",
    "        color = []\n",
    "        for i in range(num_routed_experts):\n",
    "            color.extend([i]*num_neurons)\n",
    "        axs[l].scatter(X, Y, Z, marker='o', c=color, cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    if n_dim == 2:\n",
    "        plot_one_2d_layer(all_projected_neuron[i], i)\n",
    "    elif n_dim == 3:\n",
    "        plot_one_3d_layer(all_projected_neuron[i], i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrices = ['linear_v', 'linear', 'linear_1']\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "num_neurons = grok_model.config.intermediate_size\n",
    "hidden_size = grok_model.config.hidden_size\n",
    "use_normalize = True\n",
    "\n",
    "all_projected_matrix = [[] for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    for matrix in matrices:\n",
    "        all_matrix = torch.empty(num_experts, num_neurons*hidden_size)\n",
    "        for idx in range(num_experts):\n",
    "            if matrix == 'linear_1':\n",
    "                all_matrix[idx] = getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight.T.flatten()\n",
    "            else:\n",
    "                all_matrix[idx] = getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight.flatten()\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_matrix, dim=0), torch.std(all_matrix, dim=0)\n",
    "            all_matrix = (all_matrix - mean) / std\n",
    "        pca = PCA(n_components=2, svd_solver='full')\n",
    "        projected_matrix = pca.fit_transform(all_matrix.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "        all_projected_matrix[i].append(projected_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_experts_pca')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'all_projected_matrix'), 'wb') as f:\n",
    "    pickle.dump(all_projected_matrix, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(projected_matrix_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.5))\n",
    "    for l, projected_matrix in enumerate(projected_matrix_lst):\n",
    "        X = projected_matrix[:, 0]\n",
    "        Y = projected_matrix[:, 1]\n",
    "        axs[l].scatter(X, Y, marker='o')\n",
    "        for i in range(num_experts):\n",
    "            axs[l].annotate(i, (X[i], Y[i]), fontsize=12)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_projected_matrix[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrices = ['linear_v', 'linear', 'linear_1']\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "num_neurons = grok_model.config.intermediate_size\n",
    "hidden_size = grok_model.config.hidden_size\n",
    "use_normalize = True\n",
    "n_dim = 3 # 2D or 3D space\n",
    "assert n_dim in [2, 3]\n",
    "\n",
    "save_dir = os.path.join(WORK_DIR, f'grok/grok_experts_pca_neuron/{n_dim}d')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_2d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        X = projected_neuron[:, 0]\n",
    "        Y = projected_neuron[:, 1]\n",
    "        color = []\n",
    "        for i in range(num_experts):\n",
    "            color.extend([i]*num_neurons)\n",
    "        axs[l].scatter(X, Y, marker='o', c=color, cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_3d_layer(projected_neuron_lst, layer_idx):\n",
    "    fig, axs = plt.subplots(ncols=3, layout='constrained', figsize=(9., 2.7), subplot_kw=dict(projection='3d'))\n",
    "    for l, projected_neuron in enumerate(projected_neuron_lst):\n",
    "        X = projected_neuron[:, 0]\n",
    "        Y = projected_neuron[:, 1]\n",
    "        Z = projected_neuron[:, 2]\n",
    "        color = []\n",
    "        for i in range(num_experts):\n",
    "            color.extend([i]*num_neurons)\n",
    "        axs[l].scatter(X, Y, Z, marker='o', c=color, cmap=plt.cm.Spectral)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "        \n",
    "        \n",
    "for i in range(num_layers):\n",
    "    projected_neuron_lst = []\n",
    "    for matrix in matrices:\n",
    "        all_neuron_lst = []\n",
    "        for idx in range(num_experts):\n",
    "            if matrix == 'linear_1':\n",
    "                all_neuron_lst.append(getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight.T)\n",
    "            else:\n",
    "                all_neuron_lst.append(getattr(grok_model.model.layers[i].moe_block.experts[idx], matrix).weight)\n",
    "        all_neuron = torch.cat(all_neuron_lst, dim=0)\n",
    "        if use_normalize:\n",
    "            mean, std = torch.mean(all_neuron, dim=0), torch.std(all_neuron, dim=0)\n",
    "            all_neuron = (all_neuron - mean) / std\n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        projected_neuron = pca.fit_transform(all_neuron.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "        projected_neuron_lst.append(projected_neuron)\n",
    "\n",
    "    with open(os.path.join(output_dir, f'layer{i}_projected_neuron_lst'), 'wb') as f:\n",
    "        pickle.dump(projected_neuron_lst, f)\n",
    "        \n",
    "    if n_dim == 2:\n",
    "        plot_one_2d_layer(projected_neuron_lst, i)\n",
    "    elif n_dim == 3:\n",
    "        plot_one_3d_layer(projected_neuron_lst, i)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
