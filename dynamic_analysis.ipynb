{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Dynamic Behaviours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes experiments listed below:\n",
    "- Outputs of experts\n",
    "- Norms of expert outputs and gate scores\n",
    "- Intermediate states of experts\n",
    "- Chosen experts\n",
    "\n",
    "The models have their own code blocks for each experiment. The overall logic of the code belonging to different models is alike, and the minor differences stem from the unique settings of the corresponding model.\n",
    "\n",
    "Usually, the figures are plotted in two ways: 'auto_colorbar' and 'full_colorbar'. The former allows the matplotlib methods to automatically dicide the range of the color bar for each layer. For the latter, we manually set it to be the global minimum/maximum for all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import ml_dtypes\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import functools\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import normalize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mixtral_base.modeling_moe_mistral import MixtralForCausalLM, MistralDecoderLayer, FeedForward\n",
    "from mixtral_instruct.modeling_mixtral_instruct import MixtralInstructForCausalLM\n",
    "from mistral.modeling_mistral import MistralModel, MistralMLP\n",
    "from deepseekmoe.modeling_deepseek import DeepseekForCausalLM, DeepseekDecoderLayer, DeepseekMLP, MoEGate\n",
    "from grok.modeling_grok1 import Grok1ModelForCausalLM, DecoderLayer, MoeMLP\n",
    "\n",
    "# The root directory for saving the output figures and data.\n",
    "WORK_DIR = './outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one or more cells below to load the models you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_model = MixtralForCausalLM.from_pretrained(\n",
    "    \"./ckpt/mixtral\", \n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "mixtral_tok = AutoTokenizer.from_pretrained(\"./ckpt/mixtral\")\n",
    "mixtral_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_instruct_model = MixtralInstructForCausalLM.from_pretrained(\n",
    "    \"./ckpt/mixtral-instruct\", \n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "mixtral_instruct_tok = AutoTokenizer.from_pretrained(\"./ckpt/mixtral-instruct\")\n",
    "mixtral_instruct_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = MistralModel.from_pretrained(\n",
    "    './ckpt/mistral',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "mistral_tok = AutoTokenizer.from_pretrained(\"./ckpt/mistral\")\n",
    "mistral_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_model = DeepseekForCausalLM.from_pretrained(\n",
    "    './ckpt/deepseekmoe',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "deepseek_tok = AutoTokenizer.from_pretrained(\"./ckpt/deepseekmoe\")\n",
    "deepseek_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_model = Grok1ModelForCausalLM.from_pretrained(\n",
    "    './ckpt/grok',\n",
    "    low_cpu_mem_usage=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "grok_tok = AutoTokenizer.from_pretrained(\"./ckpt/grok\")\n",
    "grok_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both the short and long sequence in this experiment. We plot the similarity heat map of each token in the short sequence, while only the averaged heat map is plotted for the long sequence. To employ the long sequence as the input, set `use_short_input=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral and Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = [('w3', 'up_proj'), ('w1', 'gate_proj'), ('w2', 'down_proj')]\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "tick_labels.append('F')\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_experts_outsim')\n",
    "if not use_short_input:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(arr_lst, all_gate_indices, layer_idx, num_tokens, range_type, global_vmin=None, global_vmax=None):\n",
    "    imlst = []\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(8.0, 2.5))\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'exp {exp1},{exp2}', fontsize=15)\n",
    "        axs[i].set_xticks(np.arange(num_experts+1), labels=tick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(np.arange(num_experts+1), labels=tick_labels, fontsize=15)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=12., fontsize=20)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=0.88)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(avg_arr, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    imlst = []\n",
    "    fig, ax = plt.subplots(ncols=1, layout='constrained', figsize=(3.5, 2.0))\n",
    "    if range_type == 'auto_colorbar':\n",
    "        im = ax.imshow(avg_arr)\n",
    "        imlst.append(im)\n",
    "    elif range_type == 'full_colorbar':\n",
    "        im = ax.imshow(avg_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "    ax.set_xticks(np.arange(num_experts+1), labels=tick_labels, fontsize=14.5)\n",
    "    ax.set_yticks(np.arange(num_experts+1), labels=tick_labels, fontsize=14.5)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=12., fontsize=20)\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=14.5)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "def get_angular_similarity(v1, v2):\n",
    "    batch_cos = torch.nn.CosineSimilarity(dim=2)\n",
    "    return 1 - (torch.acos(batch_cos(v1, v2)) / math.pi)\n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    scores = output\n",
    "    _, expert_indices = torch.topk(scores, 2, dim=-1)\n",
    "    all_gate_indices[layer_idx].append(expert_indices.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output shape = [num_tokens, hidden_dim]\n",
    "    all_expert_output[layer_idx][expert_idx] = output \n",
    "\n",
    "\n",
    "def record_ffn_output(module, input, output, layer_idx):\n",
    "    all_expert_output[layer_idx][-1] = output[0, ...]\n",
    "\n",
    "\n",
    "total_token_count = 0\n",
    "all_sim_arr = [np.zeros((num_experts+1, num_experts+1)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        # For the long sequence, use the first ten sentences only.\n",
    "        break\n",
    "    mix_enc_input = mixtral_tok.encode(sent, return_tensors='pt') # mix_enc_input is actually the same as mis_enc_input.\n",
    "    mis_enc_input = mistral_tok.encode(sent, return_tensors='pt')\n",
    "    assert mix_enc_input.shape[1] == mis_enc_input.shape[1]\n",
    "    num_tokens = mix_enc_input.shape[1]\n",
    "    total_token_count += num_tokens\n",
    "    print(s, num_tokens, total_token_count)\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=2. \n",
    "    for name, module in mixtral_model.named_modules():\n",
    "        if isinstance(module, MistralDecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    mix_output = mixtral_model(mix_enc_input)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts to ALL.\n",
    "    for i in range(num_layers):\n",
    "        mixtral_model.model.layers[i].mlp.num_experts_per_token = num_experts\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(num_layers):\n",
    "        for name, module in mixtral_model.named_modules():\n",
    "            if isinstance(module, FeedForward):\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                mix_output = mixtral_model(mix_enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                # Feed the topk=2 output of previous layer as input.\n",
    "                mix_output = mixtral_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        handles = []\n",
    "    # Revert to the original value.\n",
    "    for i in range(num_layers):\n",
    "        mixtral_model.model.layers[i].mlp.num_experts_per_token = mixtral_model.config.num_experts_per_token\n",
    "\n",
    "    # Obtain Mistral FFNs' output.\n",
    "    for name, module in mistral_model.named_modules():\n",
    "        if isinstance(module, MistralMLP):\n",
    "            layer_idx = int(name.split('.')[1])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_ffn_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mis_output = mistral_model(mis_enc_input)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    # Compute similarity between outputs of current sentence.\n",
    "    if use_short_input:\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                sim_arr = np.ones((num_experts+1, num_experts+1))\n",
    "                for k in range(num_experts+1):\n",
    "                    for l in range(k+1, num_experts+1):\n",
    "                        # Mixtral and Mistral layers can be loaded on differnet GPUs, so put them on the same device manually. \n",
    "                        sim = cos(all_expert_output[i][k][j].to('cuda:0'), all_expert_output[i][l][j].to('cuda:0')).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                        sim_arr[k][l] = sim\n",
    "                        sim_arr[l][k] = sim\n",
    "                all_sim_arr[i].append(sim_arr)\n",
    "    else:\n",
    "        output_dim = all_expert_output[0][0][0].shape[0]\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                # Reorganize recorded data to compute similarity in parallel.\n",
    "                expert_output_self = torch.empty(num_experts+1, 1, output_dim).cuda()\n",
    "                expert_output_other = torch.empty(1, num_experts+1, output_dim).cuda()\n",
    "                for k in range(num_experts+1):\n",
    "                    k = -1 if k == num_experts else k\n",
    "                    expert_output_self[k, 0] = all_expert_output[i][k][j]\n",
    "                    expert_output_other[0, k] = all_expert_output[i][k][j]\n",
    "                sim = get_angular_similarity(expert_output_self, expert_output_other).fill_diagonal_(1.) # Replace nan values due to numerical instability\n",
    "                sim = sim.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                all_sim_arr[i] += sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    # Record the maximum and minimum values for plotting.\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    for i in range(num_layers):\n",
    "        for j in range(num_tokens):\n",
    "            sim_arr = all_sim_arr[i][j]\n",
    "            curr_vmax = np.max(sim_arr)\n",
    "            curr_vmin = np.min(sim_arr)\n",
    "            if curr_vmin < global_vmin:\n",
    "                global_vmin = curr_vmin\n",
    "            if curr_vmax > global_vmax:\n",
    "                global_vmax = curr_vmax\n",
    "    \n",
    "    output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "        pickle.dump(all_sim_arr, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'auto_colorbar')\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'full_colorbar', global_vmin, global_vmax)\n",
    "\n",
    "else:\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    all_avg_arr = []\n",
    "    for i in range(num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        all_avg_arr.append(avg_arr)\n",
    "        curr_vmax = np.max(avg_arr)\n",
    "        curr_vmin = np.min(avg_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax\n",
    "    output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_avg_arr'), 'wb') as f:\n",
    "        pickle.dump(all_avg_arr, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        plot_one_layer_long_seq(all_avg_arr[i], i, 'auto_colorbar')\n",
    "        plot_one_layer_long_seq(all_avg_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['up_proj', 'gate_proj', 'down_proj']\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "tick_pos = [i for i in range(0, num_routed_experts, 8)]\n",
    "tick_labels = [str(i) for i in range(0, num_routed_experts, 8)]\n",
    "tick_pos.append(num_routed_experts)\n",
    "tick_labels.append('SE')\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_experts_outsim')\n",
    "if not use_short_input:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(arr_lst, all_gate_indices, layer_idx, num_tokens, range_type, global_vmin=None, global_vmax=None):\n",
    "    imlst = []\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(14., 4))\n",
    "    num_chosen_experts = deepseek_model.config.num_experts_per_tok\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        chosen_experts = ''\n",
    "        for j in range(num_chosen_experts):\n",
    "            chosen_experts += str(all_gate_indices[layer_idx][0][i, j])\n",
    "            if j != num_chosen_experts - 1:\n",
    "                chosen_experts += ', '\n",
    "        axs[i].set_title(f'exp {chosen_experts}', fontsize=15)\n",
    "        axs[i].set_xticks(tick_pos, labels=tick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(tick_pos, labels=tick_labels, fontsize=15)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=36)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(avg_arr, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, ax = plt.subplots(ncols=1, layout='constrained', figsize=(7.5, 3.5))\n",
    "    if range_type == 'auto_colorbar':\n",
    "        im = ax.imshow(avg_arr)\n",
    "    elif range_type == 'full_colorbar':\n",
    "        im = ax.imshow(avg_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "    ax.set_xticks(tick_pos, labels=tick_labels, fontsize=18)\n",
    "    ax.set_yticks(tick_pos, labels=tick_labels, fontsize=18)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=18., fontsize=26)\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "\n",
    "def get_angular_similarity(v1, v2):\n",
    "    batch_cos = torch.nn.CosineSimilarity(dim=2)\n",
    "    return 1 - (torch.acos(batch_cos(v1, v2)) / math.pi) \n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output shape = [num_tokens, hidden_dim]\n",
    "    if expert_idx == -1:\n",
    "        all_expert_output[layer_idx][expert_idx] = output.squeeze(dim=0)\n",
    "    else:\n",
    "        all_expert_output[layer_idx][expert_idx] = output \n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    expert_indices, expert_weights, _ = output\n",
    "    all_gate_indices[layer_idx].append(expert_indices.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "total_token_count = 0\n",
    "all_sim_arr = [np.zeros((num_routed_experts+1, num_routed_experts+1)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        # For the long sequence, use the first ten sentences only.\n",
    "        break\n",
    "    enc_input = deepseek_tok.encode(sent, return_tensors='pt').cuda()\n",
    "    num_tokens = enc_input.shape[1]\n",
    "    total_token_count += num_tokens\n",
    "    print(s, num_tokens, total_token_count)\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=6. \n",
    "    for name, module in deepseek_model.named_modules():\n",
    "        if isinstance(module, DeepseekDecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, MoEGate):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = deepseek_model(enc_input)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts.\n",
    "    for i in range(1, num_layers):\n",
    "        curr_layer = deepseek_model.model.layers[i]\n",
    "        if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "            curr_layer.mlp.num_experts_per_tok = num_routed_experts\n",
    "        if hasattr(curr_layer.mlp, 'gate'):\n",
    "            curr_layer.mlp.gate.top_k = num_routed_experts\n",
    "\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(num_layers):\n",
    "        for name, module in deepseek_model.named_modules():\n",
    "            if isinstance(module, DeepseekMLP) and 'shared_experts' in name:\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=-1) # Use -1 to represent shared experts.\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "            elif isinstance(module, DeepseekMLP) and 'experts' in name:\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                output = deepseek_model(enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                # Feed the topk=6 output of previous layer as input.\n",
    "                output = deepseek_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "    # Revert to the original value.\n",
    "    for i in range(1, num_layers):\n",
    "        curr_layer = deepseek_model.model.layers[i]\n",
    "        if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "            curr_layer.mlp.num_experts_per_tok = deepseek_model.config.num_experts_per_tok\n",
    "        if hasattr(curr_layer.mlp, 'gate'):\n",
    "            curr_layer.mlp.gate.top_k = deepseek_model.config.num_experts_per_tok\n",
    "\n",
    "    # Compute similarity between outputs of current sentence.\n",
    "    if use_short_input:\n",
    "        for i in range(1, num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                sim_arr = np.empty((num_routed_experts+1, num_routed_experts+1))\n",
    "                for k in range(num_routed_experts+1):\n",
    "                    for l in range(k, num_routed_experts+1):\n",
    "                        sim = cos(all_expert_output[i][k][j], all_expert_output[i][l][j]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "                        sim_arr[k][l] = sim\n",
    "                        sim_arr[l][k] = sim\n",
    "                all_sim_arr[i].append(sim_arr)\n",
    "    else:\n",
    "        output_dim = all_expert_output[1][0][0].shape[0]\n",
    "        for i in range(1, num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                # Reorganize recorded data to compute similarity in parallel.\n",
    "                expert_output_self = torch.empty(num_routed_experts+1, 1, output_dim).cuda()\n",
    "                expert_output_other = torch.empty(1, num_routed_experts+1, output_dim).cuda()\n",
    "                for k in range(num_routed_experts+1):\n",
    "                    k = -1 if k == num_routed_experts else k\n",
    "                    expert_output_self[k, 0] = all_expert_output[i][k][j]\n",
    "                    expert_output_other[0, k] = all_expert_output[i][k][j]\n",
    "                sim = get_angular_similarity(expert_output_self, expert_output_other).fill_diagonal_(1.) # Replace nan values due to numerical instability\n",
    "                sim = sim.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                all_sim_arr[i] += sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    # Record the maximum and minimum values for plotting.\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    for i in range(num_layers):\n",
    "        for j in range(num_tokens):\n",
    "            sim_arr = all_sim_arr[i][j]\n",
    "            curr_vmax = np.max(sim_arr)\n",
    "            curr_vmin = np.min(sim_arr)\n",
    "            if curr_vmin < global_vmin:\n",
    "                global_vmin = curr_vmin\n",
    "            if curr_vmax > global_vmax:\n",
    "                global_vmax = curr_vmax\n",
    "\n",
    "    output_dict = {'global_vmax': global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "        pickle.dump(all_sim_arr, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)  \n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'auto_colorbar')\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'full_colorbar', global_vmin, global_vmax)\n",
    "\n",
    "else:\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    all_avg_arr = [[] for _ in range(num_layers)]\n",
    "    for i in range(1, num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        all_avg_arr[i].append(avg_arr)\n",
    "        curr_vmax = np.max(avg_arr)\n",
    "        curr_vmin = np.min(avg_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax\n",
    "\n",
    "    output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_avg_arr'), 'wb') as f:\n",
    "        pickle.dump(all_avg_arr, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        plot_one_layer_long_seq(all_avg_arr[i][0], i, 'auto_colorbar')\n",
    "        plot_one_layer_long_seq(all_avg_arr[i][0], i, 'full_colorbar', global_vmin, global_vmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "matrices = ['linear_v', 'linear', 'linear_1'] # up, gate, down\n",
    "normalize_output = False\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_experts_outsim')\n",
    "if not use_short_input:\n",
    "    save_dir += '_average'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(arr_lst, all_gate_indices, layer_idx, num_tokens, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(8., 2.5))\n",
    "    imlst = []\n",
    "    for i, sim_arr in enumerate(arr_lst):\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(sim_arr)\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(sim_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'exp {exp1},{exp2}', fontsize=16)\n",
    "        axs[i].set_xticks(np.arange(num_experts), labels=tick_labels, fontsize=16)\n",
    "        axs[i].set_yticks(np.arange(num_experts), labels=tick_labels, fontsize=16)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=12., fontsize=20)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=.88)\n",
    "    cbar.ax.tick_params(labelsize=16)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(avg_arr, layer_idx, range_type, global_vmin=None, global_vmax=None):\n",
    "    fig, ax = plt.subplots(ncols=1, layout='constrained', figsize=(4., 2.0))\n",
    "    if range_type == 'auto_colorbar':\n",
    "        im = ax.imshow(avg_arr)\n",
    "    elif range_type == 'full_colorbar':\n",
    "        im = ax.imshow(avg_arr, vmin=global_vmin, vmax=global_vmax)\n",
    "    ax.set_xticks(np.arange(num_experts), labels=tick_labels, fontsize=14.5)\n",
    "    ax.set_yticks(np.arange(num_experts), labels=tick_labels, fontsize=14.5)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=12., fontsize=20)\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=14.5)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "def get_angular_similarity(v1, v2):\n",
    "    batch_cos = torch.nn.CosineSimilarity(dim=2)\n",
    "    return 1 - (torch.acos(batch_cos(v1, v2)) / math.pi) \n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    # output[0] shape: (num_tokens, hidden_dim)\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    router_logits = output\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    # shape: (num_tokens, topk)\n",
    "    routing_weights, selected_experts = torch.topk(routing_weights, 2, dim=-1)\n",
    "    all_gate_indices[layer_idx].append(selected_experts.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output shape: (num_tokens, hidden_dim)\n",
    "    if normalize_output:\n",
    "        all_expert_output[layer_idx][expert_idx] = normalize(output, p=2.0, dim=1)\n",
    "    else: \n",
    "        all_expert_output[layer_idx][expert_idx] = output \n",
    "\n",
    "\n",
    "total_token_count = 0\n",
    "all_sim_arr = [np.zeros((num_routed_experts+1, num_routed_experts+1)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        # For the long sequence, use the first ten sentences only.\n",
    "        break\n",
    "    enc_input = grok_tok.encode(sent, return_tensors='pt').cuda()\n",
    "    attention_mask = torch.ones_like(enc_input)\n",
    "    inputs = {\n",
    "        \"input_ids\": grok_tok(sent, return_tensors='pt').input_ids.cuda(),\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"max_new_tokens\": 1,\n",
    "    }\n",
    "    num_tokens = enc_input.shape[1]\n",
    "    total_token_count += num_tokens\n",
    "    print(s, num_tokens, total_token_count)\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=2. \n",
    "    for name, module in grok_model.named_modules():\n",
    "        if isinstance(module, DecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    output = grok_model.generate(**inputs)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts to ALL.\n",
    "    for i in range(num_layers):\n",
    "        grok_model.model.layers[i].moe_block.top_k = num_experts\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(num_layers):\n",
    "        for name, module in grok_model.named_modules():\n",
    "            if isinstance(module, MoeMLP):\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                output = grok_model(enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                # Feed the topk=2 output of previous layer as input.\n",
    "                output = grok_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        handles = []\n",
    "    # Revert to the original value.\n",
    "    for i in range(num_layers):\n",
    "        grok_model.model.layers[i].moe_block.top_k = grok_model.config.num_experts_per_tok\n",
    "\n",
    "    # Compute similarity between outputs of current sentence.\n",
    "    if use_short_input:\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                sim_arr = np.ones((num_experts, num_experts))\n",
    "                for k in range(num_experts):\n",
    "                    for l in range(k+1, num_experts):\n",
    "                        sim = cos(all_expert_output[i][k][j], all_expert_output[i][l][j]).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                        sim_arr[k][l] = sim\n",
    "                        sim_arr[l][k] = sim\n",
    "                all_sim_arr[i].append(sim_arr)\n",
    "    else:\n",
    "        output_dim = all_expert_output[0][0][0].shape[0]\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                # Reorganize recorded data to compute similarity in parallel.\n",
    "                expert_output_self = torch.empty(num_experts, 1, output_dim).cuda()\n",
    "                expert_output_other = torch.empty(1, num_experts, output_dim).cuda()\n",
    "                for k in range(num_experts):\n",
    "                    expert_output_self[k, 0] = all_expert_output[i][k][j]\n",
    "                    expert_output_other[0, k] = all_expert_output[i][k][j]\n",
    "                sim = get_angular_similarity(expert_output_self, expert_output_other).fill_diagonal_(1.) # Replace nan values due to numerical instability\n",
    "                sim = sim.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "                all_sim_arr[i] += sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    # Record the maximum and minimum values for plotting.\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    for i in range(num_layers):\n",
    "        for j in range(num_tokens):\n",
    "            sim_arr = all_sim_arr[i][j]\n",
    "            curr_vmax = np.max(sim_arr)\n",
    "            curr_vmin = np.min(sim_arr)\n",
    "            if curr_vmin < global_vmin:\n",
    "                global_vmin = curr_vmin\n",
    "            if curr_vmax > global_vmax:\n",
    "                global_vmax = curr_vmax\n",
    "    \n",
    "    output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_sim_arr'), 'wb') as f:\n",
    "        pickle.dump(all_sim_arr, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'auto_colorbar')\n",
    "        plot_one_layer_short_seq(all_sim_arr[i], all_gate_indices, i, num_tokens, 'full_colorbar', global_vmin, global_vmax)\n",
    "\n",
    "else:\n",
    "    global_vmax, global_vmin = -1 * math.inf, math.inf\n",
    "    all_avg_arr = []\n",
    "    for i in range(num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        all_avg_arr.append(avg_arr)\n",
    "        curr_vmax = np.max(avg_arr)\n",
    "        curr_vmin = np.min(avg_arr)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n",
    "        if curr_vmax > global_vmax:\n",
    "            global_vmax = curr_vmax\n",
    "    output_dict = {'global_vmax':global_vmax, 'global_vmin':global_vmin}\n",
    "    with open(os.path.join(output_dir, 'all_avg_arr'), 'wb') as f:\n",
    "        pickle.dump(all_avg_arr, f)\n",
    "    with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "        pickle.dump(output_dict, f)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        avg_arr = all_sim_arr[i] / total_token_count\n",
    "        plot_one_layer_long_seq(all_avg_arr[i], i, 'auto_colorbar')\n",
    "        plot_one_layer_long_seq(all_avg_arr[i], i, 'full_colorbar', global_vmin, global_vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms of Expert Outputs and Gate Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both the short and long sequence in this experiment. We plot the norm and gate score of every expert for each token in the short sequence, while only the rank counting is plotted for the long sequence. To employ the long sequence as the input, set `use_short_input=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_expert_norm')\n",
    "if not use_short_input:\n",
    "    save_dir += '_count'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, layer_idx, num_tokens):\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(22.0, 2.8))\n",
    "    for i in range(num_tokens):\n",
    "        # Plot the norm of feature vectors output by experts.\n",
    "        norm_lst = []\n",
    "        for j in range(num_experts):\n",
    "            norm_lst.append(all_expert_output[layer_idx][j][i])\n",
    "        im1 = axs[i].bar(np.arange(num_experts)*2-0.35, norm_lst, label='Norm', width=0.6)\n",
    "        # Plot the gate scores.\n",
    "        twin_ax = axs[i].twinx()\n",
    "        im2 = twin_ax.bar(np.arange(num_experts)*2, all_gate_scores[layer_idx][0][i, :], tick_label=tick_labels, \n",
    "                          color='darkorange', align='edge', label='Score', width=0.5)\n",
    "        axs[i].set_xticks(np.arange(num_experts)*2, labels=tick_labels, fontsize=18)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=22)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'exp {exp1},{exp2}', fontsize=18)\n",
    "        axs[i].legend(loc='upper left', fontsize=12)\n",
    "        twin_ax.legend(loc='upper right', fontsize=12)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(rankings_counts, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(6.5, 4.0))\n",
    "    bar_width = 0.1\n",
    "    x = np.arange(num_experts)\n",
    "    for i in range(num_experts):\n",
    "        offset = bar_width * i\n",
    "        im = ax.bar(x+offset, rankings_counts[i, :], bar_width, label=f'score rank {i+1}')\n",
    "    ax.set_xticks(x+3.5*bar_width, [str(i+1) for i in range(num_experts)], fontsize=13)\n",
    "    ax.tick_params(axis='y', labelsize=11)\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.set_xlabel('Expert output norm ranking', fontsize=15)\n",
    "    ax.set_ylabel('Count of gate score ranking', fontsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer{layer_idx}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    scores = output\n",
    "    _, expert_indices = torch.topk(scores, 2, dim=-1, sorted=True)\n",
    "    all_gate_indices[layer_idx].append(expert_indices.cpu().detach().numpy())\n",
    "    all_gate_scores[layer_idx].append(scores.softmax(dim=-1).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output size = [num_tokens, hidden_dim]\n",
    "    all_expert_output[layer_idx][expert_idx] = torch.norm(output, dim=1).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "\n",
    "\n",
    "token_count = 0\n",
    "rankings_counts = [np.zeros((num_experts, num_experts)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        break\n",
    "    enc_input = mixtral_tok.encode(sent, return_tensors=\"pt\").cuda()\n",
    "    num_tokens = enc_input.shape[1]\n",
    "    token_count += num_tokens\n",
    "    print(s, num_tokens, token_count)\n",
    "    all_gate_scores = [[] for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=2. \n",
    "    for name, module in mixtral_model.named_modules():\n",
    "        if isinstance(module, MistralDecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mix_output = mixtral_model(enc_input)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts to ALL.\n",
    "    for i in range(num_layers):\n",
    "        mixtral_model.model.layers[i].mlp.num_experts_per_token = num_experts\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(num_layers):\n",
    "        for name, module in mixtral_model.named_modules():\n",
    "            if isinstance(module, FeedForward):\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                mix_output = mixtral_model(enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "            # Feed the topk=2 output of previous layer as input.\n",
    "                mix_output = mixtral_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        handles = []\n",
    "    # Revert to the original value.\n",
    "    for i in range(num_layers):\n",
    "        mixtral_model.model.layers[i].mlp.num_experts_per_token = mixtral_model.config.num_experts_per_token\n",
    "\n",
    "    if not use_short_input:\n",
    "        # Count the norm-score ranking pairs.\n",
    "        for i in range(num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                curr_token_output = np.array([])\n",
    "                for k in range(num_experts):\n",
    "                    curr_token_output = np.append(curr_token_output, all_expert_output[i][k][j])\n",
    "                curr_gate_score = all_gate_scores[i][0][j, :]\n",
    "                norm_rank = np.argsort(curr_token_output)\n",
    "                score_rank = np.argsort(curr_gate_score)\n",
    "                # Replace the values with the corresponding rankings.\n",
    "                for rank, idx in enumerate(norm_rank):\n",
    "                    curr_token_output[idx] = rank\n",
    "                for rank, idx in enumerate(score_rank):\n",
    "                    curr_gate_score[idx] = rank\n",
    "                for row, col in zip(curr_gate_score.tolist(), curr_token_output.tolist()):\n",
    "                    rankings_counts[i][int(row), int(col)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    with open(os.path.join(output_dir, 'all_gate_scores'), 'wb') as f:\n",
    "        pickle.dump(all_gate_scores, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'all_expert_output'), 'wb') as f:\n",
    "        pickle.dump(all_expert_output, f)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, i, num_tokens)\n",
    "\n",
    "else:\n",
    "    with open(os.path.join(output_dir, 'rankings_counts'), 'wb') as f:\n",
    "        pickle.dump(rankings_counts, f)\n",
    "    # Plot layer one by one.\n",
    "    for l in range(num_layers):\n",
    "        plot_one_layer_long_seq(rankings_counts[l], l)\n",
    "    # Plot all layers.\n",
    "    total_rankings_counts = rankings_counts[0]\n",
    "    for l in range(1, num_layers):\n",
    "        total_rankings_counts += rankings_counts[l]\n",
    "    plot_one_layer_long_seq(total_rankings_counts, 'ALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "\n",
    "tick_pos = [i*2-0.35 for i in range(0, num_routed_experts, 8)]\n",
    "tick_labels = [str(i) for i in range(0, num_routed_experts, 8)]\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_expert_norm')\n",
    "if not use_short_input:\n",
    "    save_dir += '_count'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, layer_idx, num_tokens):\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(36.0, 5.0))\n",
    "    num_chosen_experts = deepseek_model.config.num_experts_per_tok\n",
    "    for i in range(num_tokens):\n",
    "        norm_lst = []\n",
    "        for j in range(num_routed_experts):\n",
    "            norm_lst.append(all_expert_output[layer_idx][j][i])\n",
    "        im1 = axs[i].bar(np.arange(num_routed_experts)*2-0.35, norm_lst, label='Norm', width=0.6)\n",
    "        axs[i].set_xticks(tick_pos, labels=tick_labels, fontsize=22)\n",
    "        # Plot the gate scores.\n",
    "        twin_ax = axs[i].twinx()\n",
    "        im2 = twin_ax.bar(np.arange(num_routed_experts)*2, all_gate_scores[layer_idx][0][i, :], \n",
    "                          color='darkorange', align='edge', label='Score', width=0.5)\n",
    "        chosen_experts = ''\n",
    "        for j in range(num_chosen_experts):\n",
    "            chosen_experts += str(all_gate_indices[layer_idx][0][i, j])\n",
    "            if j != num_chosen_experts - 1:\n",
    "                chosen_experts += ', '\n",
    "        axs[i].set_title(f'exp {chosen_experts}', fontsize=22)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=18., fontsize=36)\n",
    "        axs[i].legend(loc='upper left', fontsize=18)\n",
    "        twin_ax.legend(loc='upper right', fontsize=18)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(rankings_counts, layer_idx):\n",
    "    nrows = 8\n",
    "    fig, axs = plt.subplots(nrows=nrows, layout='constrained', figsize=(24., 16.))\n",
    "    num_expert_per_row = int(num_routed_experts/nrows)\n",
    "    bar_width = 0.0125\n",
    "    x = np.arange(num_expert_per_row)\n",
    "    for i in range(nrows):\n",
    "        for j in range(num_routed_experts):\n",
    "            offset = bar_width * j\n",
    "            im = axs[i].bar(x+offset, rankings_counts[j, num_expert_per_row*i:num_expert_per_row*(i+1)], bar_width)\n",
    "        axs[i].set_xticks(x, [str(n+1) for n in range(num_expert_per_row*i, num_expert_per_row*(i+1))], fontsize=20)\n",
    "        axs[i].tick_params(axis='y', labelsize=20)\n",
    "        axs[i].set_ylim([0., np.max(rankings_counts)+1])\n",
    "        if i % 4 == 1:\n",
    "            axs[i].set_ylabel('Count of gate score ranking', fontsize=25)\n",
    "    axs[-1].set_xlabel('Expert output norm ranking', fontsize=25)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer{layer_idx}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output: Size([7, 2048]) = [num_tokens, hidden_dim]\n",
    "    all_expert_output[layer_idx][expert_idx] = torch.norm(output, dim=1).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    bsz, seq_len, h = input[0].shape        \n",
    "    logits = F.linear(input[0].view(-1, h), module.weight, None)\n",
    "    scores = logits.softmax(dim=-1)\n",
    "    topk_weight, topk_idx = torch.topk(scores, k=6, dim=-1, sorted=True)\n",
    "    all_gate_scores[layer_idx].append(scores.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "    all_gate_indices[layer_idx].append(topk_idx.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "token_count = 0\n",
    "rankings_counts = [np.zeros((num_routed_experts, num_routed_experts)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        break\n",
    "    enc_input = deepseek_tok.encode(sent, return_tensors='pt').cuda()\n",
    "    num_tokens = enc_input.shape[1]\n",
    "    token_count += num_tokens\n",
    "    print(s, num_tokens, token_count)\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    all_gate_scores = [[] for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=6. \n",
    "    for name, module in deepseek_model.named_modules():\n",
    "        if isinstance(module, DeepseekDecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, MoEGate):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = deepseek_model(enc_input)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts.\n",
    "    for i in range(1, num_layers):\n",
    "        curr_layer = deepseek_model.model.layers[i]\n",
    "        if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "            curr_layer.mlp.num_experts_per_tok = num_routed_experts\n",
    "        if hasattr(curr_layer.mlp, 'gate'):\n",
    "            curr_layer.mlp.gate.top_k = num_routed_experts\n",
    "\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(1, num_layers):\n",
    "        for name, module in deepseek_model.named_modules():\n",
    "            if isinstance(module, DeepseekMLP) and 'shared_experts' in name:\n",
    "                continue\n",
    "            elif isinstance(module, DeepseekMLP) and 'experts' in name:\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        with torch.no_grad():\n",
    "            # Feed the topk=6 output of previous layer as input.\n",
    "            output = deepseek_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "    # Revert to the original value.\n",
    "    for i in range(1, num_layers):\n",
    "        curr_layer = deepseek_model.model.layers[i]\n",
    "        if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "            curr_layer.mlp.num_experts_per_tok = deepseek_model.config.num_experts_per_tok\n",
    "        if hasattr(curr_layer.mlp, 'gate'):\n",
    "            curr_layer.mlp.gate.top_k = deepseek_model.config.num_experts_per_tok\n",
    "\n",
    "    if not use_short_input:\n",
    "        for i in range(1, num_layers):\n",
    "            for j in range(num_tokens):\n",
    "                curr_token_output = np.array([])\n",
    "                for k in range(num_routed_experts):\n",
    "                    curr_token_output = np.append(curr_token_output, all_expert_output[i][k][j])\n",
    "                curr_gate_score = all_gate_scores[i][0][j, :]\n",
    "                norm_rank = np.argsort(curr_token_output)\n",
    "                score_rank = np.argsort(curr_gate_score)\n",
    "                # Replace the values with the corresponding rankings.\n",
    "                for rank, idx in enumerate(norm_rank):\n",
    "                    curr_token_output[idx] = rank\n",
    "                for rank, idx in enumerate(score_rank):\n",
    "                    curr_gate_score[idx] = rank\n",
    "                for row, col in zip(curr_gate_score.tolist(), curr_token_output.tolist()):\n",
    "                    rankings_counts[i][int(row), int(col)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    with open(os.path.join(output_dir, 'all_gate_scores'), 'wb') as f:\n",
    "        pickle.dump(all_gate_scores, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'all_expert_output'), 'wb') as f:\n",
    "        pickle.dump(all_expert_output, f)\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, i, num_tokens)\n",
    "\n",
    "else:\n",
    "    with open(os.path.join(output_dir, 'rankings_counts'), 'wb') as f:\n",
    "        pickle.dump(rankings_counts, f)\n",
    "    # Plot layer one by one.\n",
    "    for l in range(1, num_layers):\n",
    "        plot_one_layer_long_seq(rankings_counts[l], l)\n",
    "    # Plot all layers.\n",
    "    total_rankings_counts = rankings_counts[1]\n",
    "    for l in range(2, num_layers):\n",
    "        total_rankings_counts += rankings_counts[l]\n",
    "    plot_one_layer_long_seq(total_rankings_counts, 'ALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "use_short_input = True # Set False to use the long sequence.\n",
    "sentence_lst = []\n",
    "if use_short_input:\n",
    "    raw_input = \"As an open source alternative to\"\n",
    "    sentence_lst.append(raw_input)\n",
    "else:\n",
    "    with open('./wikitext103_test.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "        for row in csv_reader:\n",
    "            sentences = row[0].split('\\n')\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent.startswith('=') or sent == '':\n",
    "                    continue\n",
    "                sentence_lst.append(sent)\n",
    "\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "tick_labels = [str(i) for i in range(num_experts)]\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_expert_norm')\n",
    "if not use_short_input:\n",
    "    save_dir += '_count'\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, layer_idx, num_tokens):\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(22.0, 2.8))\n",
    "    for i in range(num_tokens):\n",
    "        norm_lst = []\n",
    "        for j in range(num_experts):\n",
    "            norm_lst.append(all_expert_output[layer_idx][j][i])\n",
    "        im1 = axs[i].bar(np.arange(num_experts)*2-0.35, norm_lst, label='Norm', width=0.6)\n",
    "        # Plot the gate scores.\n",
    "        twin_ax = axs[i].twinx()\n",
    "        im2 = twin_ax.bar(np.arange(num_experts)*2, all_gate_scores[layer_idx][0][i, :], tick_label=tick_labels, \n",
    "                          color='darkorange', align='edge', label='Score', width=0.5)\n",
    "        axs[i].set_xticks(np.arange(num_experts)*2, labels=tick_labels, fontsize=18)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'exp {exp1},{exp2}', fontsize=18)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=22)\n",
    "        axs[i].legend(loc='upper left', fontsize=12)\n",
    "        twin_ax.legend(loc='upper right', fontsize=12)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_one_layer_long_seq(rankings_counts, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(6.5, 4.0))\n",
    "    bar_width = 0.1\n",
    "    x = np.arange(num_experts)\n",
    "    for i in range(num_experts):\n",
    "        offset = bar_width * i\n",
    "        im = ax.bar(x+offset, rankings_counts[i, :], bar_width)\n",
    "    ax.set_xticks(x+3.5*bar_width, [str(i+1) for i in range(num_experts)], fontsize=13)\n",
    "    ax.tick_params(axis='y', labelsize=11)\n",
    "    ax.set_xlabel('Expert output norm ranking', fontsize=15)\n",
    "    ax.set_ylabel('Count of gate score ranking', fontsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer{layer_idx}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    # output[0] shape: (num_tokens, hidden_dim)\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    router_logits = output\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    # shape: (num_tokens, topk)\n",
    "    _, selected_experts = torch.topk(routing_weights, 2, dim=-1)\n",
    "    all_gate_indices[layer_idx].append(selected_experts.cpu().detach().numpy())\n",
    "    all_gate_scores[layer_idx].append(routing_weights.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def record_expert_output(module, input, output, layer_idx, expert_idx):\n",
    "    # output shape: (num_tokens, hidden_dim)\n",
    "    all_expert_output[layer_idx][expert_idx] = torch.norm(output, dim=1).float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "\n",
    "\n",
    "token_count = 0\n",
    "rankings_counts = [np.zeros((num_experts, num_experts)) for _ in range(num_layers)]\n",
    "for s, sent in enumerate(sentence_lst):\n",
    "    if s == 10:\n",
    "        break\n",
    "    enc_input = grok_tok.encode(sent, return_tensors='pt').cuda()\n",
    "    attention_mask = torch.ones_like(enc_input)\n",
    "    inputs = {\n",
    "        \"input_ids\": grok_tok(sent, return_tensors='pt').input_ids.cuda(),\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"max_new_tokens\": 1,\n",
    "    }\n",
    "    num_tokens = enc_input.shape[1]\n",
    "    token_count += num_tokens\n",
    "    print(s, num_tokens, token_count)\n",
    "    all_layer_output = [[] for _ in range(num_layers)]\n",
    "    all_expert_output = [{} for _ in range(num_layers)]\n",
    "    all_gate_scores = [[] for _ in range(num_layers)]\n",
    "    all_gate_indices = [[] for _ in range(num_layers)]\n",
    "    handles = []\n",
    "\n",
    "    # Obtain the original output feature vectors of experts \n",
    "    # and gate choices when topk=2. \n",
    "    for name, module in grok_model.named_modules():\n",
    "        if isinstance(module, DecoderLayer):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "        elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            handles.append(module.register_forward_hook(\n",
    "                functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "            ))\n",
    "\n",
    "    output = grok_model.generate(**inputs)\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "\n",
    "    # Modify the number of chosen experts to ALL.\n",
    "    for i in range(num_layers):\n",
    "        grok_model.model.layers[i].moe_block.top_k = num_experts\n",
    "    # Iterate over the layers and register a hook once a time.\n",
    "    for i in range(num_layers):\n",
    "        for name, module in grok_model.named_modules():\n",
    "            if isinstance(module, MoeMLP):\n",
    "                layer_idx = int(name.split('.')[2])\n",
    "                if layer_idx == i:\n",
    "                    expert_idx = int(name.split('.')[-1])\n",
    "                    handles.append(module.register_forward_hook(\n",
    "                        functools.partial(record_expert_output, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                    ))\n",
    "                elif layer_idx > i:\n",
    "                    break\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                output = grok_model(enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                # Feed the topk=2 output of previous layer as input.\n",
    "                output = grok_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        handles = []\n",
    "    # Revert to the original value.\n",
    "    for i in range(num_layers):\n",
    "        grok_model.model.layers[i].moe_block.top_k = grok_model.config.num_experts_per_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "if use_short_input:\n",
    "    with open(os.path.join(output_dir, 'all_gate_scores'), 'wb') as f:\n",
    "        pickle.dump(all_gate_scores, f)\n",
    "    with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "        pickle.dump(all_gate_indices, f)\n",
    "    with open(os.path.join(output_dir, 'all_expert_output'), 'wb') as f:\n",
    "        pickle.dump(all_expert_output, f)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        plot_one_layer_short_seq(all_gate_scores, all_gate_indices, all_expert_output, i, num_tokens)\n",
    "\n",
    "else:\n",
    "    with open(os.path.join(output_dir, 'rankings_counts'), 'wb') as f:\n",
    "        pickle.dump(rankings_counts, f)\n",
    "    # Plot layer one by one.\n",
    "    for l in range(num_layers):\n",
    "        plot_one_layer_long_seq(rankings_counts[l], l)\n",
    "    # Plot all layers.\n",
    "    total_rankings_counts = rankings_counts[0]\n",
    "    for l in range(1, num_layers):\n",
    "        total_rankings_counts += rankings_counts[l]\n",
    "    plot_one_layer_long_seq(total_rankings_counts, 'ALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate States of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the short sequence is used in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral and Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "raw_input = \"As an open source alternative to\"\n",
    "mix_enc_input = mixtral_tok.encode(raw_input, return_tensors='pt') # mix_enc_input is actually the same as mis_enc_input.\n",
    "mis_enc_input = mistral_tok.encode(raw_input, return_tensors='pt')\n",
    "\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "intermediate_size = mixtral_model.config.intermediate_size\n",
    "num_tokens = mix_enc_input.shape[1]\n",
    "all_layer_output = [[] for _ in range(num_layers)]\n",
    "all_expert_act = [{} for _ in range(num_layers)]\n",
    "all_gate_indices = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    scores = output\n",
    "    _, expert_indices = torch.topk(scores, 2, dim=-1, sorted=True)\n",
    "    all_gate_indices[layer_idx].append(expert_indices.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16))\n",
    "\n",
    "\n",
    "def record_expert_act(module, input, output, layer_idx, expert_idx):\n",
    "    # act_neurons size = [num_tokens, intermediate_size]\n",
    "    act_neurons = F.silu(module.w1(input[0]))\n",
    "    all_expert_act[layer_idx][expert_idx] = act_neurons.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "\n",
    "\n",
    "def record_ffn_output(module, input, output, layer_idx):\n",
    "    # act_neurons size = [1, num_tokens, intermediate_size]\n",
    "    act_neurons = F.silu(module.gate_proj(input[0]))\n",
    "    all_expert_act[layer_idx][-1] = act_neurons.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "\n",
    "\n",
    "# Obtain the original output feature vectors of experts \n",
    "# and gate choices when topk=2. \n",
    "for name, module in mixtral_model.named_modules():\n",
    "    if isinstance(module, MistralDecoderLayer):\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "    elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    mix_output = mixtral_model(mix_enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "handles = []\n",
    "\n",
    "# Modify the number of chosen experts to ALL.\n",
    "for i in range(num_layers):\n",
    "    mixtral_model.model.layers[i].mlp.num_experts_per_token = num_experts\n",
    "# Iterate over the layers and register a hook once a time.\n",
    "for i in range(num_layers):\n",
    "    for name, module in mixtral_model.named_modules():\n",
    "        if isinstance(module, FeedForward):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            if layer_idx == i:\n",
    "                expert_idx = int(name.split('.')[-1])\n",
    "                handles.append(module.register_forward_hook(\n",
    "                    functools.partial(record_expert_act, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                ))\n",
    "            elif layer_idx > i:\n",
    "                break\n",
    "    if i == 0:\n",
    "        with torch.no_grad():\n",
    "            mix_output = mixtral_model(mix_enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "    else: \n",
    "        with torch.no_grad():\n",
    "            # Feed the topk=2 output of previous layer as input.\n",
    "            mix_output = mixtral_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "\n",
    "# Revert to the original value.\n",
    "for i in range(num_layers):\n",
    "    mixtral_model.model.layers[i].mlp.num_experts_per_token = mixtral_model.config.num_experts_per_token\n",
    "\n",
    "# Obtain Mistral FFNs' output.\n",
    "for name, module in mistral_model.named_modules():\n",
    "    if isinstance(module, MistralMLP):\n",
    "        layer_idx = int(name.split('.')[1])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_ffn_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    mis_output = mistral_model(mis_enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "    \n",
    "global_vmin = math.inf\n",
    "for i in range(num_layers):\n",
    "    for act in all_expert_act[i].values():\n",
    "        curr_vmin = np.min(act)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "xtick_labels = [str(i) for i in range(0, intermediate_size, 4000)]\n",
    "ytick_labels = [str(i) for i in range(num_experts)]\n",
    "ytick_labels.append('F')\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_experts_inter')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dict = {'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_expert_act'), 'wb') as f:\n",
    "    pickle.dump(all_expert_act, f)\n",
    "with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "    pickle.dump(all_gate_indices, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_act, all_gate_indices, layer_idx, range_type, global_vmin=None):\n",
    "    fig, axs = plt.subplots(nrows=3, layout='constrained', figsize=(16.0, 7.0))\n",
    "    imlst = []\n",
    "    for i in range(num_tokens):\n",
    "        curr_map = np.empty((num_experts+1, intermediate_size))\n",
    "        for j in range(num_experts):\n",
    "            curr_map[j] = all_expert_act[layer_idx][j][i, :]\n",
    "        curr_map[-1] = all_expert_act[layer_idx][-1][0, i, :]\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto')\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto', vmin=global_vmin, vmax=1.0)\n",
    "        axs[i].set_xticks(np.arange(0, intermediate_size, 4000), labels=xtick_labels, fontsize=13)\n",
    "        axs[i].set_yticks(np.arange(num_experts+1), labels=ytick_labels, fontsize=13)\n",
    "        axs[i].set_yticks(np.arange(-.5, num_experts+1, 1), minor=True)\n",
    "        axs[i].tick_params(axis='y', which='minor', length=0)\n",
    "        axs[i].grid(axis='y', which='minor', color='k', linestyle='-', linewidth=.2)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'expert {exp1},{exp2}', fontsize=16)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    fig.suptitle(f'Layer {layer_idx}', fontsize=22)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'auto_colorbar')\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'full_colorbar', global_vmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "raw_input = \"As an open source alternative to\"\n",
    "enc_input = deepseek_tok.encode(raw_input, return_tensors='pt').cuda()\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "intermediate_size = deepseek_model.config.moe_intermediate_size\n",
    "num_tokens = enc_input.shape[1]\n",
    "all_layer_output = [[] for _ in range(num_layers)]\n",
    "all_expert_act = [{} for _ in range(num_layers)]\n",
    "all_gate_indices = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_expert_act(module, input, output, layer_idx, expert_idx):\n",
    "    # act_neurons size = [num_tokens, hidden_dim]\n",
    "    act_neurons = F.silu(module.gate_proj(input[0]))\n",
    "    all_expert_act[layer_idx][expert_idx] = act_neurons.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16) \n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    expert_indices, expert_weights, _ = output\n",
    "    all_gate_indices[layer_idx].append(expert_indices.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "# Obtain the original output feature vectors of experts \n",
    "# and gate choices when topk=6. \n",
    "for name, module in deepseek_model.named_modules():\n",
    "    if isinstance(module, DeepseekDecoderLayer):\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "    elif isinstance(module, MoEGate):\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = deepseek_model(enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "handles = []\n",
    "\n",
    "# Modify the number of chosen experts.\n",
    "for i in range(1, num_layers):\n",
    "    curr_layer = deepseek_model.model.layers[i]\n",
    "    if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "        curr_layer.mlp.num_experts_per_tok = num_routed_experts\n",
    "    if hasattr(curr_layer.mlp, 'gate'):\n",
    "        curr_layer.mlp.gate.top_k = num_routed_experts\n",
    "\n",
    "# Iterate over the layers and register a hook once a time.\n",
    "for i in range(1, num_layers):\n",
    "    for name, module in deepseek_model.named_modules():\n",
    "        if isinstance(module, DeepseekMLP) and '.experts' in name:\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            if layer_idx == i:\n",
    "                expert_idx = int(name.split('.')[-1])\n",
    "                handles.append(module.register_forward_hook(\n",
    "                    functools.partial(record_expert_act, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                ))\n",
    "            elif layer_idx > i:\n",
    "                break\n",
    "    with torch.no_grad():\n",
    "        # Feed the topk=6 output of previous layer as input.\n",
    "        output = deepseek_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "# Revert to the original value.\n",
    "for i in range(1, num_layers):\n",
    "    curr_layer = deepseek_model.model.layers[i]\n",
    "    if hasattr(curr_layer.mlp, 'num_experts_per_tok'):\n",
    "        curr_layer.mlp.num_experts_per_tok = deepseek_model.config.num_experts_per_tok\n",
    "    if hasattr(curr_layer.mlp, 'gate'):\n",
    "        curr_layer.mlp.gate.top_k = deepseek_model.config.num_experts_per_tok\n",
    "\n",
    "global_vmin = math.inf\n",
    "for i in range(num_layers):\n",
    "    for act in all_expert_act[i].values():\n",
    "        curr_vmin = np.min(act)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "xtick_labels = [str(i) for i in range(0, intermediate_size, 400)]\n",
    "ytick_pos = [i for i in range(0, num_routed_experts, 4)]\n",
    "ytick_labels = [str(i) for i in range(0, num_routed_experts, 4)]\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_experts_inter')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dict = {'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_expert_act'), 'wb') as f:\n",
    "    pickle.dump(all_expert_act, f)\n",
    "with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "    pickle.dump(all_gate_indices, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_act, all_gate_indices, layer_idx, range_type, global_vmin=None):\n",
    "    fig, axs = plt.subplots(ncols=num_tokens, layout='constrained', figsize=(32.0, 5.0))\n",
    "    num_chosen_experts = deepseek_model.config.num_experts_per_tok\n",
    "    imlst = []\n",
    "    for i in range(num_tokens):\n",
    "        curr_map = np.empty((num_routed_experts, intermediate_size))\n",
    "        for j in range(num_routed_experts):\n",
    "            curr_map[j] = all_expert_act[layer_idx][j][i, :]\n",
    "        chosen_experts = ''\n",
    "        for j in range(num_chosen_experts):\n",
    "            chosen_experts += str(all_gate_indices[layer_idx][0][i, j])\n",
    "            if j != num_chosen_experts - 1:\n",
    "                chosen_experts += ', '\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto')\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto', vmin=global_vmin, vmax=1.0)\n",
    "        axs[i].set_xticks(np.arange(0, intermediate_size, 400), labels=xtick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(ytick_pos, labels=ytick_labels, fontsize=15)\n",
    "        axs[i].set_yticks(np.arange(-.5, num_routed_experts, 1), minor=True)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Layer {layer_idx}', labelpad=16., fontsize=30)\n",
    "        axs[i].tick_params(axis='y', which='minor', length=0)\n",
    "        axs[i].grid(axis='y', which='minor', color='k', linestyle='-', linewidth=.2)\n",
    "        axs[i].set_title(f'exp {chosen_experts}', fontsize=18)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'auto_colorbar')\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'full_colorbar', global_vmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input.\n",
    "raw_input = \"As an open source alternative to\"\n",
    "enc_input = grok_tok.encode(raw_input, return_tensors='pt').cuda()\n",
    "attention_mask = torch.ones_like(enc_input)\n",
    "inputs = {\n",
    "    \"input_ids\": grok_tok(raw_input, return_tensors='pt').input_ids.cuda(),\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"max_new_tokens\": 1,\n",
    "}\n",
    "\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "intermediate_size = grok_model.config.intermediate_size\n",
    "num_tokens = enc_input.shape[1]\n",
    "all_layer_output = [[] for _ in range(num_layers)]\n",
    "all_expert_act = [{} for _ in range(num_layers)]\n",
    "all_gate_indices = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "\n",
    "\n",
    "def record_layer_output(module, input, output, layer_idx):\n",
    "    all_layer_output[layer_idx].append(output[0])\n",
    "\n",
    "\n",
    "def record_gate_output(module, input, output, layer_idx):  \n",
    "    router_logits = output\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    # shape: (num_tokens, topk)\n",
    "    routing_weights, selected_experts = torch.topk(routing_weights, 2, dim=-1)\n",
    "    all_gate_indices[layer_idx].append(selected_experts.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def record_expert_act(module, input, output, layer_idx, expert_idx):\n",
    "    # act_neurons shape: (num_tokens, intermediate_size)\n",
    "    act_neurons = F.gelu(module.linear(input[0]))\n",
    "    all_expert_act[layer_idx][expert_idx] = act_neurons.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16)\n",
    "\n",
    "\n",
    "# Obtain the original output feature vectors of experts \n",
    "# and gate choices when topk=2. \n",
    "for name, module in grok_model.named_modules():\n",
    "    if isinstance(module, DecoderLayer):\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_layer_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "    elif isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "        layer_idx = int(name.split('.')[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_gate_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "output = grok_model.generate(**inputs)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "handles = []\n",
    "\n",
    "# Modify the number of chosen experts to ALL.\n",
    "for i in range(num_layers):\n",
    "    grok_model.model.layers[i].moe_block.top_k = num_experts\n",
    "# Iterate over the layers and register a hook once a time.\n",
    "for i in range(num_layers):\n",
    "    for name, module in grok_model.named_modules():\n",
    "        if isinstance(module, MoeMLP):\n",
    "            layer_idx = int(name.split('.')[2])\n",
    "            if layer_idx == i:\n",
    "                expert_idx = int(name.split('.')[-1])\n",
    "                handles.append(module.register_forward_hook(\n",
    "                    functools.partial(record_expert_act, layer_idx=layer_idx, expert_idx=expert_idx)\n",
    "                ))\n",
    "            elif layer_idx > i:\n",
    "                break\n",
    "    if i == 0:\n",
    "        with torch.no_grad():\n",
    "            output = grok_model(enc_input, decoder_layer_idx=i, use_cache=False) # Set use_cache=False to prevent error.\n",
    "    else: \n",
    "        with torch.no_grad():\n",
    "            # Feed the topk=2 output of previous layer as input.\n",
    "            output = grok_model(inputs_embeds=all_layer_output[i-1][0], decoder_layer_idx=i, use_cache=False) \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    handles = []\n",
    "# Revert to the original value.\n",
    "for i in range(num_layers):\n",
    "    grok_model.model.layers[i].moe_block.top_k = grok_model.config.num_experts_per_tok\n",
    "\n",
    "global_vmin = math.inf\n",
    "for i in range(num_layers):\n",
    "    for act in all_expert_act[i].values():\n",
    "        curr_vmin = np.min(act)\n",
    "        if curr_vmin < global_vmin:\n",
    "            global_vmin = curr_vmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot.\n",
    "xtick_labels = [str(i) for i in range(0, intermediate_size, 10000)]\n",
    "ytick_labels = [str(i) for i in range(num_experts)]\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_experts_inter')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(os.path.join(plot_dir, 'auto_colorbar'), exist_ok=True)\n",
    "os.makedirs(os.path.join(plot_dir, 'full_colorbar'), exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dict = {'global_vmin':global_vmin}\n",
    "with open(os.path.join(output_dir, 'all_expert_act'), 'wb') as f:\n",
    "    pickle.dump(all_expert_act, f)\n",
    "with open(os.path.join(output_dir, 'all_gate_indices'), 'wb') as f:\n",
    "    pickle.dump(all_gate_indices, f)\n",
    "with open(os.path.join(output_dir, 'output_dict'), 'wb') as f:\n",
    "    pickle.dump(output_dict, f)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_act, all_gate_indices, layer_idx, range_type, global_vmin=None):\n",
    "    fig, axs = plt.subplots(nrows=num_tokens, layout='constrained', figsize=(16.0, 14.0))\n",
    "    imlst = []\n",
    "    for i in range(num_tokens):\n",
    "        curr_map = np.empty((num_experts, intermediate_size))\n",
    "        for j in range(num_experts):\n",
    "            curr_map[j] = all_expert_act[layer_idx][j][i, :]\n",
    "        if range_type == 'auto_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto')\n",
    "            imlst.append(im)\n",
    "        elif range_type == 'full_colorbar':\n",
    "            im = axs[i].imshow(curr_map, aspect='auto', vmin=global_vmin, vmax=1.0)\n",
    "        axs[i].set_xticks(np.arange(0, intermediate_size, 10000), labels=xtick_labels, fontsize=12)\n",
    "        axs[i].set_yticks(np.arange(num_experts), labels=ytick_labels, fontsize=14)\n",
    "        axs[i].set_yticks(np.arange(-.5, num_experts, 1), minor=True)\n",
    "        axs[i].tick_params(axis='y', which='minor', length=0)\n",
    "        axs[i].grid(axis='y', which='minor', color='k', linestyle='-', linewidth=.2)\n",
    "        exp1, exp2 = all_gate_indices[layer_idx][0][i, 0], all_gate_indices[layer_idx][0][i, 1]\n",
    "        axs[i].set_title(f'expert {exp1},{exp2}', fontsize=16)\n",
    "    if range_type == 'auto_colorbar':\n",
    "        local_vmin = min(img.get_array().min() for img in imlst)\n",
    "        local_vmax = max(img.get_array().max() for img in imlst)\n",
    "        norm = colors.Normalize(vmin=local_vmin, vmax=local_vmax)\n",
    "        for img in imlst:\n",
    "            img.set_norm(norm)\n",
    "    fig.suptitle(f'Layer {layer_idx}', fontsize=22)\n",
    "    cbar = fig.colorbar(im, ax=axs, shrink=1.)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    plt.savefig(os.path.join(plot_dir, range_type, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'auto_colorbar')\n",
    "    plot_one_layer(all_expert_act, all_gate_indices, i, 'full_colorbar', global_vmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we utilize another input containing about 64 tokens. In addition to the base model of Mixtral (Mixtral-Base), we include its instruct version (Mixtral-Instruct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = \"As an open source alternative to Chat GPT, I do not have personal opinions. However, I can provide objective information about Chat GPT's capabilities and limitations based on its architecture and training data. Chat GPT is a powerful language model based on the GPT (Generative Pre-trained Transformer\"\n",
    "enc_input = mixtral_tok.encode(raw_input, return_tensors=\"pt\").cuda()\n",
    "\n",
    "num_layers = mixtral_model.config.num_hidden_layers\n",
    "num_experts = mixtral_model.config.num_experts\n",
    "num_tokens = enc_input.shape[1]\n",
    "gate_outputs = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "xtick_labels = [mixtral_tok.decode(t) for t in enc_input[0]]\n",
    "ytick_labels = [str(i) for i in range(num_experts)]\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_gate_choice')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_weights, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(14.0, 3.))\n",
    "    im = ax.imshow(all_expert_weights, cmap=mlp.colormaps['Blues'], vmin=0., vmax=1.)\n",
    "    ax.set_xticks(np.arange(num_tokens), labels=xtick_labels, rotation='vertical', fontsize=13)\n",
    "    ax.set_yticks(np.arange(num_experts), labels=ytick_labels, fontsize=13)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    l, b, w, h = ax.get_position().bounds\n",
    "    ax.set_position([l, b, w, h+0.3])\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def record_output(module, input, output, layer_idx):  \n",
    "    scores = output\n",
    "    expert_weights, expert_indices = torch.topk(scores, 2, dim=-1)\n",
    "    expert_weights = expert_weights.softmax(dim=-1)\n",
    "    gate_outputs[layer_idx].append((expert_weights.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16),\n",
    "                                    expert_indices.cpu().detach().numpy()))\n",
    "    \n",
    "    \n",
    "for name, module in mixtral_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "        layer_idx = int(name.split(\".\")[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mixtral_model(enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "with open(os.path.join(output_dir, 'gate_outputs'), 'wb') as f:\n",
    "    pickle.dump(gate_outputs, f)\n",
    "\n",
    "for i, gate_output in enumerate(gate_outputs):\n",
    "    expert_weights, expert_indices = gate_output[0]\n",
    "    all_expert_weights = np.zeros((num_tokens, num_experts))\n",
    "    all_expert_weights[np.arange(0, num_tokens), expert_indices.T] = expert_weights.T\n",
    "    plot_one_layer(all_expert_weights.T, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = \"As an open source alternative to Chat GPT, I do not have personal opinions. However, I can provide objective information about Chat GPT's capabilities and limitations based on its architecture and training data. Chat GPT is a powerful language model based on the GPT (Generative Pre-trained Transformer\"\n",
    "enc_input = mixtral_instruct_tok.encode(raw_input, return_tensors=\"pt\")\n",
    "\n",
    "num_layers = mixtral_instruct_model.config.num_hidden_layers\n",
    "num_experts = mixtral_instruct_model.config.num_local_experts\n",
    "num_tokens = enc_input.shape[1]\n",
    "xtick_labels = [mixtral_instruct_tok.decode(t) for t in enc_input[0]]\n",
    "ytick_labels = [str(i) for i in range(num_experts)]\n",
    "gate_outputs = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "save_dir = os.path.join(WORK_DIR, 'mixtral/mixtral_instuct_gate_choice')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_weights, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(14.0, 3.))\n",
    "    im = ax.imshow(all_expert_weights, cmap=mlp.colormaps['Blues'])\n",
    "    ax.set_xticks(np.arange(num_tokens), labels=xtick_labels, rotation='vertical', fontsize=13)\n",
    "    ax.set_yticks(np.arange(num_experts), labels=ytick_labels, fontsize=13)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    l, b, w, h = ax.get_position().bounds\n",
    "    ax.set_position([l, b, w, h+0.3])\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def record_output(module, input, output, layer_idx):  \n",
    "    scores = output\n",
    "    expert_weights, expert_indices = torch.topk(scores, 2, dim=-1)\n",
    "    expert_weights = expert_weights.softmax(dim=-1)\n",
    "    gate_outputs[layer_idx].append((expert_weights.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16), \n",
    "                                    expert_indices.cpu().detach().numpy()))\n",
    "\n",
    "    \n",
    "for name, module in mixtral_instruct_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "        layer_idx = int(name.split(\".\")[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mixtral_instruct_model(enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "with open(os.path.join(output_dir, 'gate_outputs'), 'wb') as f:\n",
    "    pickle.dump(gate_outputs, f)\n",
    "\n",
    "for i, gate_output in enumerate(gate_outputs):\n",
    "    expert_weights, expert_indices = gate_output[0]\n",
    "    all_expert_weights = np.zeros((num_tokens, num_experts))\n",
    "    all_expert_weights[np.arange(0, num_tokens), expert_indices.T] = expert_weights.T\n",
    "    plot_one_layer(all_expert_weights.T, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = \"As an open source alternative to Chat GPT, I do not have personal opinions. However, I can provide objective information about Chat GPT's capabilities and limitations based on its architecture and training data. Chat GPT is a powerful language model based on the GPT (Generative Pre-trained Transformer\"\n",
    "enc_input = deepseek_tok.encode(raw_input, return_tensors=\"pt\").cuda()\n",
    "\n",
    "num_layers = deepseek_model.config.num_hidden_layers\n",
    "num_routed_experts = deepseek_model.config.n_routed_experts\n",
    "num_tokens = enc_input.shape[1]\n",
    "xtick_labels = [deepseek_tok.decode(t) for t in enc_input[0]]\n",
    "ytick_labels = [str(i) for i in range(0, num_routed_experts, 4)]\n",
    "gate_outputs = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "save_dir = os.path.join(WORK_DIR, 'deepseek/deepseek_gate_choice')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_weights, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(10.0, 14.0))\n",
    "    im = ax.imshow(all_expert_weights, cmap=mlp.colormaps['Blues'])\n",
    "    ax.set_xticks(np.arange(num_tokens), labels=xtick_labels, rotation='vertical', fontsize=13.5)\n",
    "    ax.set_yticks(np.arange(0, num_routed_experts, 4), labels=ytick_labels, fontsize=15)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def record_output(module, input, output, layer_idx): \n",
    "    expert_indices, expert_weights, _ = output\n",
    "    gate_outputs[layer_idx].append((expert_weights.float().cpu().detach().numpy().astype(ml_dtypes.bfloat16), expert_indices.cpu().detach().numpy()))\n",
    "\n",
    "    \n",
    "for name, module in deepseek_model.named_modules():\n",
    "    if isinstance(module, MoEGate):\n",
    "        layer_idx = int(name.split(\".\")[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = deepseek_model(enc_input)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "with open(os.path.join(output_dir, 'gate_outputs'), 'wb') as f:\n",
    "    pickle.dump(gate_outputs, f)\n",
    "\n",
    "for i, gate_output in enumerate(gate_outputs[1:], start=1):\n",
    "    expert_weights, expert_indices = gate_output[0]\n",
    "    all_expert_weights = np.zeros((num_tokens, num_routed_experts))\n",
    "    all_expert_weights[np.arange(0, num_tokens), expert_indices.T] = expert_weights.T\n",
    "    plot_one_layer(all_expert_weights.T, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = \"As an open source alternative to Chat GPT, I do not have personal opinions. However, I can provide objective information about Chat GPT's capabilities and limitations based on its architecture and training data. Chat GPT is a powerful language model based on the GPT (Generative Pre-trained Transformer\"\n",
    "enc_input = grok_tok(raw_input, return_tensors=\"pt\").input_ids\n",
    "enc_input = enc_input.cuda()\n",
    "attention_mask = torch.ones_like(enc_input)\n",
    "inputs = {\n",
    "    \"input_ids\": enc_input,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"max_new_tokens\": 1,\n",
    "}\n",
    "\n",
    "num_layers = grok_model.config.num_hidden_layers\n",
    "num_experts = grok_model.config.num_experts\n",
    "num_tokens = enc_input.shape[1]\n",
    "xtick_labels = [grok_tok.decode(t) for t in enc_input[0]]\n",
    "ytick_labels = [str(i) for i in range(num_experts)]\n",
    "gate_outputs = [[] for _ in range(num_layers)]\n",
    "handles = []\n",
    "save_dir = os.path.join(WORK_DIR, 'grok/grok_gate_choice')\n",
    "plot_dir = os.path.join(save_dir, 'figure')\n",
    "output_dir = os.path.join(save_dir, 'data')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_one_layer(all_expert_weights, layer_idx):\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(14.0, 3.))\n",
    "    im = ax.imshow(all_expert_weights, cmap=mlp.colormaps['Blues'], vmin=0., vmax=1.)\n",
    "    ax.set_xticks(np.arange(num_tokens), labels=xtick_labels, rotation='vertical', fontsize=12)\n",
    "    ax.set_yticks(np.arange(num_experts), labels=ytick_labels, fontsize=12)\n",
    "    ax.set_ylabel(f'Layer {layer_idx}', labelpad=14., fontsize=20)\n",
    "    l, b, w, h = ax.get_position().bounds\n",
    "    ax.set_position([l, b, w, h+0.3])\n",
    "    plt.savefig(os.path.join(plot_dir, f'layer_{layer_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def record_output(module, input, output, layer_idx):\n",
    "    router_logits = output\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    # shape: (num_tokens, topk)\n",
    "    routing_weights, selected_experts = torch.topk(routing_weights, 2, dim=-1)\n",
    "    gate_outputs[layer_idx].append((routing_weights.cpu().detach().numpy(), selected_experts.cpu().detach().numpy()))\n",
    "\n",
    "    \n",
    "for name, module in grok_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and 'gate' in name:\n",
    "        layer_idx = int(name.split(\".\")[2])\n",
    "        handles.append(module.register_forward_hook(\n",
    "            functools.partial(record_output, layer_idx=layer_idx)\n",
    "        ))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = grok_model.generate(**inputs)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "with open(os.path.join(output_dir, 'gate_outputs'), 'wb') as f:\n",
    "    pickle.dump(gate_outputs, f)\n",
    "\n",
    "for i, gate_output in enumerate(gate_outputs):\n",
    "    expert_weights, expert_indices = gate_output[0]\n",
    "    all_expert_weights = np.zeros((num_tokens, num_experts))\n",
    "    all_expert_weights[np.arange(0, num_tokens), expert_indices.T] = expert_weights.T\n",
    "    plot_one_layer(all_expert_weights.T, i)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
